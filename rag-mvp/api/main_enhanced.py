#!/usr/bin/env python3
"""
RAGå¢å¼ºç‰ˆç³»ç»Ÿ - é›†æˆLangChainå’ŒChromaDB
"""

from fastapi import FastAPI, UploadFile, File, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import os
import shutil
import hashlib
import json
import sqlite3
from datetime import datetime
from pathlib import Path
import logging

# å¯¼å…¥å¢å¼ºç‰ˆæ–‡æ¡£å¤„ç†å™¨
from document_processor import EnhancedDocumentProcessor

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="RAG Enhanced System",
    description="å¢å¼ºç‰ˆRAGç³»ç»Ÿ - ä½¿ç”¨LangChainå’ŒChromaDB",
    version="2.0.0"
)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é…ç½®è·¯å¾„
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"
UPLOAD_DIR = BASE_DIR / "uploads"
DB_PATH = DATA_DIR / "rag_enhanced.db"
CHROMA_DB_DIR = BASE_DIR / "chroma_db"

# åˆ›å»ºå¿…è¦çš„ç›®å½•
DATA_DIR.mkdir(exist_ok=True)
UPLOAD_DIR.mkdir(exist_ok=True)
CHROMA_DB_DIR.mkdir(exist_ok=True)

# å…¨å±€æ–‡æ¡£å¤„ç†å™¨
doc_processor = None

# è¯·æ±‚æ¨¡å‹
class QueryRequest(BaseModel):
    query: str
    top_k: int = 5
    filter: Optional[Dict[str, Any]] = None
    
class ProcessRequest(BaseModel):
    file_path: str
    splitter_type: str = "recursive"
    metadata: Optional[Dict[str, Any]] = None

class UpdateMetadataRequest(BaseModel):
    chunk_id: str
    metadata: Dict[str, Any]

# å“åº”æ¨¡å‹
class DocumentInfo(BaseModel):
    doc_id: str
    filename: str
    upload_time: str
    status: str
    chunk_count: int
    total_chars: int

class QueryResponse(BaseModel):
    query: str
    results: List[Dict[str, Any]]
    answer: str
    sources: List[str]
    processing_time: float

def init_database():
    """åˆå§‹åŒ–SQLiteæ•°æ®åº“"""
    conn = sqlite3.connect(str(DB_PATH))
    cursor = conn.cursor()
    
    # å¢å¼ºç‰ˆæ–‡æ¡£è¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS documents (
            id TEXT PRIMARY KEY,
            filename TEXT NOT NULL,
            file_path TEXT NOT NULL,
            upload_time TEXT NOT NULL,
            status TEXT DEFAULT 'processing',
            chunk_count INTEGER DEFAULT 0,
            total_chars INTEGER DEFAULT 0,
            splitter_type TEXT DEFAULT 'recursive',
            metadata TEXT,
            processing_time REAL
        )
    ''')
    
    # æœç´¢æ—¥å¿—è¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS search_logs (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            query TEXT NOT NULL,
            results_count INTEGER,
            processing_time REAL,
            timestamp TEXT NOT NULL,
            filter_used TEXT
        )
    ''')
    
    # ç³»ç»Ÿé…ç½®è¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS system_config (
            key TEXT PRIMARY KEY,
            value TEXT,
            updated_at TEXT
        )
    ''')
    
    conn.commit()
    conn.close()
    logger.info("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨äº‹ä»¶"""
    global doc_processor
    
    print("=" * 50)
    print("ğŸš€ RAGå¢å¼ºç‰ˆç³»ç»Ÿå¯åŠ¨ä¸­...")
    print("=" * 50)
    print(f"ğŸ“ æ•°æ®ç›®å½•: {DATA_DIR}")
    print(f"ğŸ“¤ ä¸Šä¼ ç›®å½•: {UPLOAD_DIR}")
    print(f"ğŸ—„ï¸ SQLiteæ•°æ®åº“: {DB_PATH}")
    print(f"ğŸ¯ ChromaDBç›®å½•: {CHROMA_DB_DIR}")
    print("=" * 50)
    
    # åˆå§‹åŒ–æ•°æ®åº“
    init_database()
    
    # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
    doc_processor = EnhancedDocumentProcessor(
        chroma_db_path=str(CHROMA_DB_DIR),
        embedding_model="paraphrase-MiniLM-L6-v2",
        collection_name="rag_documents"
    )
    
    print("âœ… æ–‡æ¡£å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆï¼")
    print("âœ… ChromaDBå‘é‡æ•°æ®åº“å°±ç»ªï¼")
    print("=" * 50)

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "RAGå¢å¼ºç‰ˆç³»ç»Ÿ",
        "version": "2.0.0",
        "features": [
            "LangChainæ™ºèƒ½æ–‡æ¡£åˆ‡ç‰‡",
            "ChromaDBå‘é‡æ•°æ®åº“",
            "å¤šç§åˆ‡ç‰‡ç­–ç•¥",
            "é«˜çº§è¯­ä¹‰æœç´¢"
        ]
    }

@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    stats = doc_processor.get_statistics() if doc_processor else {}
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "database": "connected",
        "chroma_db": "active",
        "statistics": stats
    }

@app.post("/api/upload")
async def upload_document(
    file: UploadFile = File(...),
    splitter_type: str = Query("recursive", description="åˆ‡ç‰‡ç±»å‹: recursive, token, char")
):
    """ä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£"""
    import time
    start_time = time.time()
    
    # ç”Ÿæˆæ–‡æ¡£ID
    doc_id = hashlib.md5(f"{file.filename}_{datetime.now().isoformat()}".encode()).hexdigest()[:12]
    
    # ä¿å­˜æ–‡ä»¶
    file_path = UPLOAD_DIR / f"{doc_id}_{file.filename}"
    with open(file_path, "wb") as buffer:
        content = await file.read()
        buffer.write(content)
    
    # å¤„ç†æ–‡æ¡£
    try:
        result = doc_processor.process_and_store(
            file_path=str(file_path),
            metadata={
                "filename": file.filename,
                "upload_time": datetime.now().isoformat(),
                "file_size": len(content)
            },
            splitter_type=splitter_type
        )
        
        processing_time = time.time() - start_time
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        conn = sqlite3.connect(str(DB_PATH))
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO documents (id, filename, file_path, upload_time, status, chunk_count, total_chars, splitter_type, processing_time)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            result['doc_id'],
            file.filename,
            str(file_path),
            datetime.now().isoformat(),
            'completed',
            result['chunks_count'],
            result['total_chars'],
            splitter_type,
            processing_time
        ))
        conn.commit()
        conn.close()
        
        return {
            "success": True,
            "doc_id": result['doc_id'],
            "filename": file.filename,
            "chunks_count": result['chunks_count'],
            "total_chars": result['total_chars'],
            "processing_time": processing_time,
            "splitter_type": splitter_type
        }
        
    except Exception as e:
        logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥: {str(e)}")
        # æ¸…ç†æ–‡ä»¶
        if file_path.exists():
            file_path.unlink()
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/query")
async def query_documents(request: QueryRequest):
    """æŸ¥è¯¢æ–‡æ¡£"""
    import time
    start_time = time.time()
    
    try:
        # æ‰§è¡Œæœç´¢
        results = doc_processor.search(
            query=request.query,
            n_results=request.top_k,
            where=request.filter
        )
        
        # æ„å»ºç­”æ¡ˆï¼ˆè¿™é‡Œæ˜¯ç®€å•æ‹¼æ¥ï¼Œå®é™…åº”è¯¥ä½¿ç”¨LLMï¼‰
        if results:
            # æå–ç›¸å…³å†…å®¹
            contexts = [r['content'][:500] for r in results]
            sources = list(set([r['metadata'].get('source', 'Unknown') for r in results]))
            
            # ç®€å•çš„ç­”æ¡ˆç”Ÿæˆ
            answer = f"æ ¹æ®çŸ¥è¯†åº“ä¸­çš„ {len(results)} ä¸ªç›¸å…³ç‰‡æ®µï¼š\n\n"
            for i, context in enumerate(contexts[:3], 1):
                answer += f"{i}. {context[:200]}...\n\n"
        else:
            answer = "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚"
            sources = []
        
        processing_time = time.time() - start_time
        
        # è®°å½•æœç´¢æ—¥å¿—
        conn = sqlite3.connect(str(DB_PATH))
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO search_logs (query, results_count, processing_time, timestamp, filter_used)
            VALUES (?, ?, ?, ?, ?)
        ''', (
            request.query,
            len(results),
            processing_time,
            datetime.now().isoformat(),
            json.dumps(request.filter) if request.filter else None
        ))
        conn.commit()
        conn.close()
        
        return {
            "query": request.query,
            "results": results,
            "answer": answer,
            "sources": sources,
            "processing_time": processing_time
        }
        
    except Exception as e:
        logger.error(f"æŸ¥è¯¢å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/documents")
async def list_documents():
    """è·å–æ–‡æ¡£åˆ—è¡¨"""
    conn = sqlite3.connect(str(DB_PATH))
    cursor = conn.cursor()
    cursor.execute('''
        SELECT id, filename, upload_time, status, chunk_count, total_chars, splitter_type, processing_time
        FROM documents
        ORDER BY upload_time DESC
    ''')
    rows = cursor.fetchall()
    conn.close()
    
    documents = []
    for row in rows:
        documents.append({
            "id": row[0],
            "filename": row[1],
            "upload_time": row[2],
            "status": row[3],
            "chunk_count": row[4],
            "total_chars": row[5],
            "splitter_type": row[6],
            "processing_time": row[7]
        })
    
    return documents

@app.delete("/api/documents/{doc_id}")
async def delete_document(doc_id: str):
    """åˆ é™¤æ–‡æ¡£"""
    try:
        # ä»å‘é‡æ•°æ®åº“åˆ é™¤
        success = doc_processor.delete_document(doc_id)
        
        if success:
            # ä»SQLiteåˆ é™¤
            conn = sqlite3.connect(str(DB_PATH))
            cursor = conn.cursor()
            cursor.execute('DELETE FROM documents WHERE id = ?', (doc_id,))
            conn.commit()
            conn.close()
            
            return {"success": True, "message": f"æ–‡æ¡£ {doc_id} å·²åˆ é™¤"}
        else:
            raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
            
    except Exception as e:
        logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/statistics")
async def get_statistics():
    """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    # è·å–ChromaDBç»Ÿè®¡
    chroma_stats = doc_processor.get_statistics() if doc_processor else {}
    
    # è·å–SQLiteç»Ÿè®¡
    conn = sqlite3.connect(str(DB_PATH))
    cursor = conn.cursor()
    
    cursor.execute('SELECT COUNT(*) FROM documents')
    doc_count = cursor.fetchone()[0]
    
    cursor.execute('SELECT COUNT(*) FROM search_logs')
    search_count = cursor.fetchone()[0]
    
    cursor.execute('SELECT AVG(processing_time) FROM documents WHERE processing_time IS NOT NULL')
    avg_processing = cursor.fetchone()[0] or 0
    
    cursor.execute('SELECT SUM(chunk_count) FROM documents')
    total_chunks = cursor.fetchone()[0] or 0
    
    conn.close()
    
    return {
        "documents_count": doc_count,
        "total_chunks": total_chunks,
        "search_count": search_count,
        "avg_processing_time": avg_processing,
        "chroma_db": chroma_stats
    }

@app.post("/api/reprocess")
async def reprocess_document(request: ProcessRequest):
    """é‡æ–°å¤„ç†å·²å­˜åœ¨çš„æ–‡æ¡£"""
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not Path(request.file_path).exists():
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        # é‡æ–°å¤„ç†
        result = doc_processor.process_and_store(
            file_path=request.file_path,
            metadata=request.metadata,
            splitter_type=request.splitter_type
        )
        
        return {
            "success": True,
            "result": result
        }
        
    except Exception as e:
        logger.error(f"é‡æ–°å¤„ç†å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/metadata")
async def update_metadata(request: UpdateMetadataRequest):
    """æ›´æ–°æ–‡æ¡£ç‰‡æ®µå…ƒæ•°æ®"""
    try:
        success = doc_processor.update_metadata(
            chunk_id=request.chunk_id,
            metadata=request.metadata
        )
        
        if success:
            return {"success": True, "message": "å…ƒæ•°æ®å·²æ›´æ–°"}
        else:
            raise HTTPException(status_code=404, detail="ç‰‡æ®µä¸å­˜åœ¨")
            
    except Exception as e:
        logger.error(f"æ›´æ–°å…ƒæ•°æ®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/search-logs")
async def get_search_logs(limit: int = Query(100, description="è¿”å›è®°å½•æ•°")):
    """è·å–æœç´¢æ—¥å¿—"""
    conn = sqlite3.connect(str(DB_PATH))
    cursor = conn.cursor()
    cursor.execute('''
        SELECT query, results_count, processing_time, timestamp, filter_used
        FROM search_logs
        ORDER BY timestamp DESC
        LIMIT ?
    ''', (limit,))
    rows = cursor.fetchall()
    conn.close()
    
    logs = []
    for row in rows:
        logs.append({
            "query": row[0],
            "results_count": row[1],
            "processing_time": row[2],
            "timestamp": row[3],
            "filter_used": json.loads(row[4]) if row[4] else None
        })
    
    return logs

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)