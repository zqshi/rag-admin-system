#!/usr/bin/env python3
"""
RAGå¢å¼ºç‰ˆç³»ç»Ÿ - é›†æˆLangChainå’ŒChromaDB
"""

from fastapi import FastAPI, UploadFile, File, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import os
import shutil
import hashlib
import json
import sqlite3
from datetime import datetime
from pathlib import Path
import logging

# å¯¼å…¥å¢å¼ºç‰ˆæ–‡æ¡£å¤„ç†å™¨å’ŒLLMç”Ÿæˆå™¨
from document_processor import EnhancedDocumentProcessor
from llm_generator import RAGGenerator

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="RAG Enhanced System",
    description="å¢å¼ºç‰ˆRAGç³»ç»Ÿ - ä½¿ç”¨LangChainå’ŒChromaDB",
    version="2.0.0"
)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é…ç½®è·¯å¾„
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"
UPLOAD_DIR = BASE_DIR / "uploads"
DB_PATH = DATA_DIR / "rag.db"
CHROMA_DB_DIR = BASE_DIR / "chroma_db"

# åˆ›å»ºå¿…è¦çš„ç›®å½•
DATA_DIR.mkdir(exist_ok=True)
UPLOAD_DIR.mkdir(exist_ok=True)
CHROMA_DB_DIR.mkdir(exist_ok=True)

# å…¨å±€æ–‡æ¡£å¤„ç†å™¨å’ŒLLMç”Ÿæˆå™¨
doc_processor = None
rag_generator = None

# è¯·æ±‚æ¨¡å‹
class QueryRequest(BaseModel):
    query: str
    top_k: int = 5
    filter: Optional[Dict[str, Any]] = None
    
class ProcessRequest(BaseModel):
    file_path: str
    splitter_type: str = "recursive"
    metadata: Optional[Dict[str, Any]] = None

class UpdateMetadataRequest(BaseModel):
    chunk_id: str
    metadata: Dict[str, Any]

# å“åº”æ¨¡å‹
class DocumentInfo(BaseModel):
    doc_id: str
    filename: str
    upload_time: str
    status: str
    chunk_count: int
    total_chars: int

class QueryResponse(BaseModel):
    query: str
    results: List[Dict[str, Any]]
    answer: str
    sources: List[str]
    processing_time: float

def init_database():
    """åˆå§‹åŒ–SQLiteæ•°æ®åº“"""
    conn = sqlite3.connect(str(DB_PATH))
    cursor = conn.cursor()
    
    # å¢å¼ºç‰ˆæ–‡æ¡£è¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS documents (
            id TEXT PRIMARY KEY,
            filename TEXT NOT NULL,
            file_path TEXT NOT NULL,
            upload_time TEXT NOT NULL,
            status TEXT DEFAULT 'processing',
            chunk_count INTEGER DEFAULT 0,
            total_chars INTEGER DEFAULT 0,
            splitter_type TEXT DEFAULT 'recursive',
            metadata TEXT,
            processing_time REAL
        )
    ''')
    
    # æœç´¢æ—¥å¿—è¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS search_logs (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            query TEXT NOT NULL,
            results_count INTEGER,
            processing_time REAL,
            timestamp TEXT NOT NULL,
            filter_used TEXT
        )
    ''')
    
    # ç³»ç»Ÿé…ç½®è¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS system_config (
            key TEXT PRIMARY KEY,
            value TEXT,
            updated_at TEXT
        )
    ''')
    
    conn.commit()
    conn.close()
    logger.info("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨äº‹ä»¶"""
    global doc_processor, rag_generator
    
    print("=" * 50)
    print("ğŸš€ RAGå¢å¼ºç‰ˆç³»ç»Ÿå¯åŠ¨ä¸­...")
    print("=" * 50)
    print(f"ğŸ“ æ•°æ®ç›®å½•: {DATA_DIR}")
    print(f"ğŸ“¤ ä¸Šä¼ ç›®å½•: {UPLOAD_DIR}")
    print(f"ğŸ—„ï¸ SQLiteæ•°æ®åº“: {DB_PATH}")
    print(f"ğŸ¯ ChromaDBç›®å½•: {CHROMA_DB_DIR}")
    print("=" * 50)
    
    # åˆå§‹åŒ–æ•°æ®åº“
    init_database()
    
    # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
    doc_processor = EnhancedDocumentProcessor(
        chroma_db_path=str(CHROMA_DB_DIR),
        embedding_model="paraphrase-MiniLM-L6-v2",
        collection_name="rag_documents"
    )
    
    # åˆå§‹åŒ–LLMç”Ÿæˆå™¨ - ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œé™çº§åˆ°ç®€å•å›ç­”
    try:
        # å°è¯•æœ¬åœ°LLM
        rag_generator = RAGGenerator(provider="local", model="llama2")
        print("âœ… LLMç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆï¼(æœ¬åœ°æ¨¡å¼)")
    except Exception as e:
        logger.warning(f"æœ¬åœ°LLMåˆå§‹åŒ–å¤±è´¥: {e}")
        try:
            # å°è¯•OpenAI
            rag_generator = RAGGenerator(provider="openai")
            print("âœ… LLMç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆï¼(OpenAIæ¨¡å¼)")
        except Exception as e:
            logger.warning(f"OpenAIåˆå§‹åŒ–å¤±è´¥: {e}")
            try:
                # å°è¯•Claude
                rag_generator = RAGGenerator(provider="claude")
                print("âœ… LLMç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆï¼(Claudeæ¨¡å¼)")
            except Exception as e:
                logger.warning(f"Claudeåˆå§‹åŒ–å¤±è´¥: {e}")
                rag_generator = None
                print("âš ï¸ LLMç”Ÿæˆå™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨ç®€å•æ¨¡å¼")
    
    print("âœ… æ–‡æ¡£å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆï¼")
    print("âœ… ChromaDBå‘é‡æ•°æ®åº“å°±ç»ªï¼")
    print("=" * 50)

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "RAGå¢å¼ºç‰ˆç³»ç»Ÿ",
        "version": "2.0.0",
        "features": [
            "LangChainæ™ºèƒ½æ–‡æ¡£åˆ‡ç‰‡",
            "ChromaDBå‘é‡æ•°æ®åº“",
            "å¤šç§åˆ‡ç‰‡ç­–ç•¥",
            "é«˜çº§è¯­ä¹‰æœç´¢"
        ]
    }

@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    stats = doc_processor.get_statistics() if doc_processor else {}
    
    # LLMçŠ¶æ€æ£€æŸ¥
    llm_status = {
        "available": rag_generator is not None,
        "provider": rag_generator.provider if rag_generator else None,
        "model": getattr(rag_generator.generator, 'model', 'unknown') if rag_generator else None
    }
    
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "database": "connected",
        "chroma_db": "active",
        "llm": llm_status,
        "components": {
            "document_processor": "ready" if doc_processor else "not_initialized",
            "vector_db": "ready" if doc_processor else "not_initialized", 
            "llm_generator": "ready" if rag_generator else "fallback_mode"
        },
        "statistics": stats
    }

@app.post("/api/upload")
async def upload_document(
    file: UploadFile = File(...),
    splitter_type: str = Query("recursive", description="åˆ‡ç‰‡ç±»å‹: recursive, token, char")
):
    """ä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£"""
    import time
    start_time = time.time()
    
    # ç”Ÿæˆæ–‡æ¡£ID
    doc_id = hashlib.md5(f"{file.filename}_{datetime.now().isoformat()}".encode()).hexdigest()[:12]
    
    # ä¿å­˜æ–‡ä»¶
    file_path = UPLOAD_DIR / f"{doc_id}_{file.filename}"
    with open(file_path, "wb") as buffer:
        content = await file.read()
        buffer.write(content)
    
    # å¤„ç†æ–‡æ¡£
    try:
        result = doc_processor.process_and_store(
            file_path=str(file_path),
            metadata={
                "filename": file.filename,
                "upload_time": datetime.now().isoformat(),
                "file_size": len(content)
            },
            splitter_type=splitter_type
        )
        
        processing_time = time.time() - start_time
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        conn = sqlite3.connect(str(DB_PATH))
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO documents (id, filename, file_path, upload_time, status, chunk_count, total_chars, splitter_type, processing_time)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            result['doc_id'],
            file.filename,
            str(file_path),
            datetime.now().isoformat(),
            'completed',
            result['chunks_count'],
            result['total_chars'],
            splitter_type,
            processing_time
        ))
        conn.commit()
        conn.close()
        
        return {
            "success": True,
            "doc_id": result['doc_id'],
            "filename": file.filename,
            "chunks_count": result['chunks_count'],
            "total_chars": result['total_chars'],
            "processing_time": processing_time,
            "splitter_type": splitter_type
        }
        
    except Exception as e:
        logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥: {str(e)}")
        # æ¸…ç†æ–‡ä»¶
        if file_path.exists():
            file_path.unlink()
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/query")
async def query_documents(request: QueryRequest):
    """æŸ¥è¯¢æ–‡æ¡£"""
    import time
    start_time = time.time()
    
    try:
        # æ‰§è¡Œæœç´¢
        results = doc_processor.search(
            query=request.query,
            n_results=request.top_k,
            where=request.filter
        )
        
        # ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ
        if rag_generator and results:
            # ä½¿ç”¨LLMç”Ÿæˆå™¨ç”Ÿæˆæ™ºèƒ½å›ç­”
            llm_result = rag_generator.generate_answer(
                query=request.query,
                retrieved_results=results,
                max_tokens=1000,
                temperature=0.7
            )
            
            answer = llm_result.get("answer", "ç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚")
            sources = llm_result.get("sources", [])
            
            # è®°å½•LLMä½¿ç”¨æƒ…å†µ
            logger.info(f"LLMç”Ÿæˆå®Œæˆ: model={llm_result.get('model')}, "
                       f"tokens={llm_result.get('tokens_used', 0)}, "
                       f"success={llm_result.get('success', False)}")
            
        elif results:
            # é™çº§ï¼šç®€å•çš„æ–‡æœ¬æ‹¼æ¥ï¼ˆæ— LLMå¯ç”¨æ—¶ï¼‰
            contexts = []
            seen_contexts = set()
            
            for r in results:
                context = r['content'][:300].strip()
                context_hash = hash(context)
                if context_hash not in seen_contexts:
                    seen_contexts.add(context_hash)
                    contexts.append(context)
            
            sources = list(set([r['metadata'].get('source', 'Unknown') for r in results]))
            sources = [s.split('/')[-1] for s in sources]  # åªä¿ç•™æ–‡ä»¶å
            
            if len(contexts) > 0:
                answer = f"åŸºäºæ£€ç´¢åˆ°çš„ {len(results)} ä¸ªæ–‡æ¡£ç‰‡æ®µï¼Œæ‰¾åˆ°ä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼š\n\n"
                for i, context in enumerate(contexts[:3], 1):
                    preview = context[:200] + "..." if len(context) > 200 else context
                    answer += f"**ç‰‡æ®µ{i}**: {preview}\n\n"
                
                if len(contexts) > 3:
                    answer += f"...ä»¥åŠå…¶ä»– {len(contexts)-3} ä¸ªç›¸å…³ç‰‡æ®µã€‚"
                    
                answer += "\n\n> âš ï¸ å½“å‰ä½¿ç”¨ç®€å•æ¨¡å¼ï¼Œå»ºè®®é…ç½®LLMè·å¾—æ›´æ™ºèƒ½çš„å›ç­”ã€‚"
            else:
                answer = "æ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£ï¼Œä½†å†…å®¹å¤„ç†å‡ºç°é—®é¢˜ã€‚"
        else:
            answer = "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨çš„é—®é¢˜ç›¸å…³çš„å†…å®¹ã€‚è¯·å°è¯•å…¶ä»–å…³é”®è¯æˆ–ä¸Šä¼ ç›¸å…³æ–‡æ¡£ã€‚"
            sources = []
        
        processing_time = time.time() - start_time
        
        # è®°å½•æœç´¢æ—¥å¿—
        conn = sqlite3.connect(str(DB_PATH))
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO search_logs (query, results_count, processing_time, timestamp, filter_used)
            VALUES (?, ?, ?, ?, ?)
        ''', (
            request.query,
            len(results),
            processing_time,
            datetime.now().isoformat(),
            json.dumps(request.filter) if request.filter else None
        ))
        conn.commit()
        conn.close()
        
        return {
            "query": request.query,
            "results": results,
            "answer": answer,
            "sources": sources,
            "processing_time": processing_time
        }
        
    except Exception as e:
        logger.error(f"æŸ¥è¯¢å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/documents")
async def list_documents():
    """è·å–æ–‡æ¡£åˆ—è¡¨"""
    conn = sqlite3.connect(str(DB_PATH))
    cursor = conn.cursor()
    cursor.execute('''
        SELECT id, filename, upload_time, status, chunk_count, total_chars, splitter_type, processing_time
        FROM documents
        ORDER BY upload_time DESC
    ''')
    rows = cursor.fetchall()
    conn.close()
    
    documents = []
    for row in rows:
        documents.append({
            "id": row[0],
            "filename": row[1],
            "upload_time": row[2],
            "status": row[3],
            "chunk_count": row[4],
            "total_chars": row[5],
            "splitter_type": row[6],
            "processing_time": row[7]
        })
    
    return documents

@app.delete("/api/documents/{doc_id}")
async def delete_document(doc_id: str):
    """åˆ é™¤æ–‡æ¡£"""
    try:
        # ä»å‘é‡æ•°æ®åº“åˆ é™¤
        success = doc_processor.delete_document(doc_id)
        
        if success:
            # ä»SQLiteåˆ é™¤
            conn = sqlite3.connect(str(DB_PATH))
            cursor = conn.cursor()
            cursor.execute('DELETE FROM documents WHERE id = ?', (doc_id,))
            conn.commit()
            conn.close()
            
            return {"success": True, "message": f"æ–‡æ¡£ {doc_id} å·²åˆ é™¤"}
        else:
            raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
            
    except Exception as e:
        logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/statistics")
async def get_statistics():
    """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    # è·å–ChromaDBç»Ÿè®¡
    chroma_stats = doc_processor.get_statistics() if doc_processor else {}
    
    # è·å–SQLiteç»Ÿè®¡
    conn = sqlite3.connect(str(DB_PATH))
    cursor = conn.cursor()
    
    cursor.execute('SELECT COUNT(*) FROM documents')
    doc_count = cursor.fetchone()[0]
    
    cursor.execute('SELECT COUNT(*) FROM search_logs')
    search_count = cursor.fetchone()[0]
    
    cursor.execute('SELECT AVG(processing_time) FROM documents WHERE processing_time IS NOT NULL')
    avg_processing = cursor.fetchone()[0] or 0
    
    cursor.execute('SELECT SUM(chunk_count) FROM documents')
    total_chunks = cursor.fetchone()[0] or 0
    
    conn.close()
    
    return {
        "documents_count": doc_count,
        "total_chunks": total_chunks,
        "search_count": search_count,
        "avg_processing_time": avg_processing,
        "chroma_db": chroma_stats
    }

@app.post("/api/reprocess")
async def reprocess_document(request: ProcessRequest):
    """é‡æ–°å¤„ç†å·²å­˜åœ¨çš„æ–‡æ¡£"""
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not Path(request.file_path).exists():
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        # é‡æ–°å¤„ç†
        result = doc_processor.process_and_store(
            file_path=request.file_path,
            metadata=request.metadata,
            splitter_type=request.splitter_type
        )
        
        return {
            "success": True,
            "result": result
        }
        
    except Exception as e:
        logger.error(f"é‡æ–°å¤„ç†å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/metadata")
async def update_metadata(request: UpdateMetadataRequest):
    """æ›´æ–°æ–‡æ¡£ç‰‡æ®µå…ƒæ•°æ®"""
    try:
        success = doc_processor.update_metadata(
            chunk_id=request.chunk_id,
            metadata=request.metadata
        )
        
        if success:
            return {"success": True, "message": "å…ƒæ•°æ®å·²æ›´æ–°"}
        else:
            raise HTTPException(status_code=404, detail="ç‰‡æ®µä¸å­˜åœ¨")
            
    except Exception as e:
        logger.error(f"æ›´æ–°å…ƒæ•°æ®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/search-logs")
async def get_search_logs(limit: int = Query(100, description="è¿”å›è®°å½•æ•°")):
    """è·å–æœç´¢æ—¥å¿—"""
    conn = sqlite3.connect(str(DB_PATH))
    cursor = conn.cursor()
    cursor.execute('''
        SELECT query, results_count, processing_time, timestamp, filter_used
        FROM search_logs
        ORDER BY timestamp DESC
        LIMIT ?
    ''', (limit,))
    rows = cursor.fetchall()
    conn.close()
    
    logs = []
    for row in rows:
        logs.append({
            "query": row[0],
            "results_count": row[1],
            "processing_time": row[2],
            "timestamp": row[3],
            "filter_used": json.loads(row[4]) if row[4] else None
        })
    
    return logs

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)