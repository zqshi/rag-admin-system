#!/usr/bin/env python3
"""
RAG MVPç³»ç»Ÿ - æç®€å¯è¿è¡Œç‰ˆæœ¬
ç›®æ ‡ï¼š2å°æ—¶å†…å®ç°æ–‡æ¡£ä¸Šä¼ ã€å‘é‡åŒ–ã€æ™ºèƒ½é—®ç­”
"""

from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import List, Optional
import os
import shutil
import hashlib
import json
import sqlite3
from datetime import datetime
from pathlib import Path

# å‘é‡åŒ–å’Œæ£€ç´¢
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

# æ–‡æ¡£å¤„ç†
import PyPDF2

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="RAG MVP System",
    description="å¿«é€ŸåŸå‹ - æ–‡æ¡£æ™ºèƒ½é—®ç­”ç³»ç»Ÿ",
    version="0.1.0"
)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”è¯¥è®¾ç½®å…·ä½“çš„åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é…ç½®è·¯å¾„
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"
UPLOAD_DIR = BASE_DIR / "uploads"
DB_PATH = DATA_DIR / "rag.db"

# åˆ›å»ºå¿…è¦çš„ç›®å½•
DATA_DIR.mkdir(exist_ok=True)
UPLOAD_DIR.mkdir(exist_ok=True)

# å…¨å±€å˜é‡
model = None
index = None
dimension = 384  # MiniLM-L6-v2çš„è¾“å‡ºç»´åº¦

def init_vector_model():
    """åˆå§‹åŒ–å‘é‡æ¨¡å‹"""
    global model, index
    print("æ­£åœ¨åŠ è½½å‘é‡æ¨¡å‹...")
    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')
    index = faiss.IndexFlatL2(dimension)
    print("å‘é‡æ¨¡å‹åŠ è½½å®Œæˆï¼")

def init_database():
    """åˆå§‹åŒ–SQLiteæ•°æ®åº“"""
    conn = sqlite3.connect(str(DB_PATH))
    cursor = conn.cursor()
    
    # æ–‡æ¡£è¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS documents (
            id TEXT PRIMARY KEY,
            filename TEXT NOT NULL,
            file_path TEXT NOT NULL,
            upload_time TEXT NOT NULL,
            status TEXT DEFAULT 'processing',
            chunk_count INTEGER DEFAULT 0,
            total_chars INTEGER DEFAULT 0
        )
    ''')
    
    # æ–‡æ¡£ç‰‡æ®µè¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS chunks (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            doc_id TEXT NOT NULL,
            content TEXT NOT NULL,
            position INTEGER NOT NULL,
            vector_id INTEGER,
            char_count INTEGER,
            FOREIGN KEY (doc_id) REFERENCES documents(id)
        )
    ''')
    
    # æœç´¢æ—¥å¿—è¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS search_logs (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            query TEXT NOT NULL,
            result_count INTEGER,
            processing_time REAL,
            created_at TEXT NOT NULL
        )
    ''')
    
    conn.commit()
    conn.close()
    print("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆï¼")

def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
    """
    ç®€å•çš„å›ºå®šå¤§å°æ–‡æœ¬åˆ†ç‰‡
    """
    if not text or not text.strip():
        return []
    
    chunks = []
    text = text.strip()
    text_len = len(text)
    
    # å¦‚æœæ–‡æœ¬å¾ˆçŸ­ï¼Œç›´æ¥ä½œä¸ºä¸€ä¸ªç‰‡æ®µ
    if text_len <= chunk_size:
        return [text]
    
    start = 0
    while start < text_len:
        end = min(start + chunk_size, text_len)
        chunk = text[start:end].strip()
        
        if chunk:
            chunks.append(chunk)
        
        # å¦‚æœæ˜¯æœ€åä¸€ä¸ªç‰‡æ®µï¼Œé€€å‡º
        if end >= text_len:
            break
            
        start = end - overlap
    
    return chunks

def extract_pdf_text(file_path: Path) -> str:
    """æå–PDFæ–‡æœ¬å†…å®¹"""
    text = ""
    try:
        with open(file_path, 'rb') as file:
            try:
                pdf_reader = PyPDF2.PdfReader(file)
                num_pages = len(pdf_reader.pages)
                
                for page_num in range(num_pages):
                    try:
                        page = pdf_reader.pages[page_num]
                        page_text = page.extract_text()
                        if page_text:
                            text += page_text + "\n"
                    except Exception as page_error:
                        print(f"è·³è¿‡ç¬¬{page_num + 1}é¡µ: {page_error}")
                        continue
                        
                print(f"æˆåŠŸæå–PDF: {file_path.name}, å…±{num_pages}é¡µ")
            except Exception as pdf_error:
                # å¦‚æœæ˜¯åŠ å¯†PDFï¼Œå°è¯•ç”¨ç©ºå¯†ç è§£å¯†
                print(f"PDFå¯èƒ½è¢«åŠ å¯†ï¼Œå°è¯•è§£å¯†: {pdf_error}")
                pdf_reader = PyPDF2.PdfReader(file)
                if pdf_reader.is_encrypted:
                    try:
                        pdf_reader.decrypt('')  # å°è¯•ç©ºå¯†ç 
                        for page in pdf_reader.pages:
                            page_text = page.extract_text()
                            if page_text:
                                text += page_text + "\n"
                    except:
                        raise HTTPException(status_code=400, detail="PDFæ–‡ä»¶è¢«åŠ å¯†ï¼Œæ— æ³•è¯»å–")
                else:
                    raise
                    
    except Exception as e:
        print(f"PDFæå–é”™è¯¯ ({file_path.name}): {str(e)}")
        # è¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ç”¨æˆ·çŸ¥é“é—®é¢˜
        return f"[PDFæå–å¤±è´¥: {str(e)}]"
    
    return text if text else "[PDFå†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–]"

def extract_text_from_file(file_path: Path, filename: str) -> str:
    """æ ¹æ®æ–‡ä»¶ç±»å‹æå–æ–‡æœ¬"""
    ext = filename.lower().split('.')[-1]
    
    if ext == 'pdf':
        return extract_pdf_text(file_path)
    elif ext in ['txt', 'md', 'text', 'markdown']:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            return f.read()
    else:
        raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {ext}")

# æ•°æ®æ¨¡å‹
class QueryRequest(BaseModel):
    query: str
    top_k: int = 5

class QueryResponse(BaseModel):
    answer: str
    sources: List[dict]
    processing_time: float
    total_results: int

class DocumentInfo(BaseModel):
    id: str
    filename: str
    upload_time: str
    status: str
    chunk_count: int
    total_chars: int

# APIè·¯ç”±
@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–"""
    init_database()
    init_vector_model()

@app.get("/")
async def root():
    """APIæ ¹è·¯å¾„"""
    return {
        "message": "RAG MVP System Running",
        "version": "0.1.0",
        "endpoints": {
            "upload": "/api/upload",
            "query": "/api/query", 
            "documents": "/api/documents",
            "health": "/api/health"
        }
    }

@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "index_size": index.ntotal if index else 0,
        "timestamp": datetime.now().isoformat()
    }

@app.post("/api/upload")
async def upload_document(file: UploadFile = File(...)):
    """ä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£"""
    start_time = datetime.now()
    
    # éªŒè¯æ–‡ä»¶
    if not file.filename:
        raise HTTPException(status_code=400, detail="æ–‡ä»¶åä¸èƒ½ä¸ºç©º")
    
    # ç”Ÿæˆå”¯ä¸€ID
    doc_id = hashlib.md5(
        f"{file.filename}{datetime.now().isoformat()}".encode()
    ).hexdigest()[:12]
    
    try:
        # ä¿å­˜æ–‡ä»¶
        file_path = UPLOAD_DIR / f"{doc_id}_{file.filename}"
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        # è®°å½•åˆ°æ•°æ®åº“
        conn = sqlite3.connect(str(DB_PATH))
        cursor = conn.cursor()
        
        cursor.execute(
            """INSERT INTO documents 
               (id, filename, file_path, upload_time, status) 
               VALUES (?, ?, ?, ?, ?)""",
            (doc_id, file.filename, str(file_path), 
             datetime.now().isoformat(), "processing")
        )
        conn.commit()
        
        # æå–æ–‡æœ¬
        try:
            text = extract_text_from_file(file_path, file.filename)
        except Exception as e:
            cursor.execute(
                "UPDATE documents SET status = ? WHERE id = ?",
                ("failed", doc_id)
            )
            conn.commit()
            conn.close()
            return {
                "status": "error",
                "message": f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}",
                "doc_id": doc_id,
                "filename": file.filename
            }
        
        # æ£€æŸ¥æ˜¯å¦æå–å¤±è´¥
        if text and text.startswith("[PDFæå–å¤±è´¥"):
            cursor.execute(
                "UPDATE documents SET status = ? WHERE id = ?",
                ("failed", doc_id)
            )
            conn.commit()
            conn.close()
            return {
                "status": "error",
                "message": text,
                "doc_id": doc_id,
                "filename": file.filename
            }
        
        if not text or not text.strip() or text == "[PDFå†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–]":
            cursor.execute(
                "UPDATE documents SET status = ? WHERE id = ?",
                ("empty", doc_id)
            )
            conn.commit()
            conn.close()
            return {
                "status": "warning",
                "message": "æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–",
                "doc_id": doc_id,
                "filename": file.filename
            }
        
        # æ–‡æœ¬åˆ†ç‰‡
        chunks = chunk_text(text, chunk_size=500, overlap=50)
        
        # å‘é‡åŒ–å¹¶å­˜å‚¨
        for i, chunk in enumerate(chunks):
            # ç”Ÿæˆå‘é‡
            embedding = model.encode([chunk])[0]
            
            # æ·»åŠ åˆ°FAISSç´¢å¼•
            vector_id = index.ntotal
            index.add(np.array([embedding], dtype=np.float32))
            
            # å­˜å‚¨åˆ°æ•°æ®åº“
            cursor.execute(
                """INSERT INTO chunks 
                   (doc_id, content, position, vector_id, char_count) 
                   VALUES (?, ?, ?, ?, ?)""",
                (doc_id, chunk, i, vector_id, len(chunk))
            )
        
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€
        cursor.execute(
            """UPDATE documents 
               SET status = ?, chunk_count = ?, total_chars = ? 
               WHERE id = ?""",
            ("completed", len(chunks), len(text), doc_id)
        )
        conn.commit()
        conn.close()
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return {
            "status": "success",
            "doc_id": doc_id,
            "filename": file.filename,
            "chunks_created": len(chunks),
            "total_chars": len(text),
            "processing_time": f"{processing_time:.2f}ç§’"
        }
        
    except Exception as e:
        # æ¸…ç†å¤±è´¥çš„ä¸Šä¼ 
        if file_path.exists():
            file_path.unlink()
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/query", response_model=QueryResponse)
async def query_documents(request: QueryRequest):
    """æ™ºèƒ½é—®ç­”æŸ¥è¯¢"""
    start_time = datetime.now()
    
    if not request.query or not request.query.strip():
        raise HTTPException(status_code=400, detail="æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡æ¡£
    if index.ntotal == 0:
        return QueryResponse(
            answer="ç³»ç»Ÿä¸­è¿˜æ²¡æœ‰ä»»ä½•æ–‡æ¡£ï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£ã€‚",
            sources=[],
            processing_time=0,
            total_results=0
        )
    
    try:
        # æŸ¥è¯¢å‘é‡åŒ–
        query_embedding = model.encode([request.query])[0]
        
        # FAISSæœç´¢
        k = min(request.top_k, index.ntotal)
        distances, indices = index.search(
            np.array([query_embedding], dtype=np.float32), k
        )
        
        # è·å–å¯¹åº”çš„æ–‡æœ¬ç‰‡æ®µ
        conn = sqlite3.connect(str(DB_PATH))
        cursor = conn.cursor()
        
        sources = []
        contexts = []
        
        for idx, distance in zip(indices[0], distances[0]):
            cursor.execute(
                """SELECT c.content, c.position, d.filename, c.char_count
                   FROM chunks c 
                   JOIN documents d ON c.doc_id = d.id 
                   WHERE c.vector_id = ?""",
                (int(idx),)
            )
            result = cursor.fetchone()
            
            if result:
                content, position, filename, char_count = result
                score = float(1 / (1 + distance))  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
                
                sources.append({
                    "content": content[:300] + "..." if len(content) > 300 else content,
                    "filename": filename,
                    "position": position + 1,
                    "score": round(score, 3),
                    "char_count": char_count
                })
                contexts.append(content)
        
        # è®°å½•æœç´¢æ—¥å¿—
        processing_time = (datetime.now() - start_time).total_seconds()
        cursor.execute(
            """INSERT INTO search_logs 
               (query, result_count, processing_time, created_at) 
               VALUES (?, ?, ?, ?)""",
            (request.query, len(sources), processing_time, 
             datetime.now().isoformat())
        )
        conn.commit()
        conn.close()
        
        # ç”Ÿæˆç­”æ¡ˆï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”æ¥å…¥LLMï¼‰
        if contexts:
            # ç®€å•æ‹¼æ¥æœ€ç›¸å…³çš„å†…å®¹ä½œä¸ºç­”æ¡ˆ
            answer = f"æ ¹æ®çŸ¥è¯†åº“ä¸­çš„ {len(contexts)} ä¸ªç›¸å…³ç‰‡æ®µï¼š\n\n"
            
            # å–å‰3ä¸ªæœ€ç›¸å…³çš„ç‰‡æ®µ
            for i, ctx in enumerate(contexts[:3], 1):
                preview = ctx[:200] + "..." if len(ctx) > 200 else ctx
                answer += f"{i}. {preview}\n\n"
                
            answer += f"\nğŸ’¡ æç¤ºï¼šè¿™æ˜¯åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„æ£€ç´¢ç»“æœï¼Œå®Œæ•´å†…å®¹è¯·æŸ¥çœ‹æºæ–‡ä»¶ã€‚"
        else:
            answer = "æœªæ‰¾åˆ°ä¸æ‚¨æŸ¥è¯¢ç›¸å…³çš„å†…å®¹ã€‚"
        
        return QueryResponse(
            answer=answer,
            sources=sources,
            processing_time=round(processing_time, 3),
            total_results=len(sources)
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¤±è´¥: {str(e)}")

@app.get("/api/documents", response_model=List[DocumentInfo])
async def list_documents():
    """è·å–æ‰€æœ‰æ–‡æ¡£åˆ—è¡¨"""
    conn = sqlite3.connect(str(DB_PATH))
    cursor = conn.cursor()
    
    cursor.execute(
        """SELECT id, filename, upload_time, status, chunk_count, total_chars 
           FROM documents 
           ORDER BY upload_time DESC"""
    )
    documents = cursor.fetchall()
    conn.close()
    
    return [
        DocumentInfo(
            id=doc[0],
            filename=doc[1],
            upload_time=doc[2],
            status=doc[3],
            chunk_count=doc[4] or 0,
            total_chars=doc[5] or 0
        )
        for doc in documents
    ]

@app.delete("/api/documents/{doc_id}")
async def delete_document(doc_id: str):
    """åˆ é™¤æ–‡æ¡£"""
    conn = sqlite3.connect(str(DB_PATH))
    cursor = conn.cursor()
    
    # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨
    cursor.execute("SELECT file_path FROM documents WHERE id = ?", (doc_id,))
    result = cursor.fetchone()
    
    if not result:
        conn.close()
        raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
    
    file_path = Path(result[0])
    
    # åˆ é™¤æ–‡ä»¶
    if file_path.exists():
        file_path.unlink()
    
    # åˆ é™¤æ•°æ®åº“è®°å½•
    cursor.execute("DELETE FROM chunks WHERE doc_id = ?", (doc_id,))
    cursor.execute("DELETE FROM documents WHERE id = ?", (doc_id,))
    conn.commit()
    conn.close()
    
    # æ³¨æ„ï¼šFAISSç´¢å¼•ä¸­çš„å‘é‡æ— æ³•å•ç‹¬åˆ é™¤ï¼Œéœ€è¦é‡å»ºç´¢å¼•
    # è¿™æ˜¯MVPç‰ˆæœ¬çš„é™åˆ¶ï¼Œç”Ÿäº§ç‰ˆæœ¬åº”è¯¥ä½¿ç”¨æ”¯æŒåˆ é™¤çš„å‘é‡æ•°æ®åº“
    
    return {"status": "success", "message": f"æ–‡æ¡£ {doc_id} å·²åˆ é™¤"}

@app.get("/api/stats")
async def get_statistics():
    """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    conn = sqlite3.connect(str(DB_PATH))
    cursor = conn.cursor()
    
    # æ–‡æ¡£ç»Ÿè®¡
    cursor.execute("SELECT COUNT(*), SUM(chunk_count), SUM(total_chars) FROM documents WHERE status = 'completed'")
    doc_stats = cursor.fetchone()
    
    # æœç´¢ç»Ÿè®¡
    cursor.execute("SELECT COUNT(*), AVG(processing_time) FROM search_logs")
    search_stats = cursor.fetchone()
    
    conn.close()
    
    return {
        "documents": {
            "total": doc_stats[0] or 0,
            "total_chunks": doc_stats[1] or 0,
            "total_chars": doc_stats[2] or 0
        },
        "searches": {
            "total": search_stats[0] or 0,
            "avg_time": round(search_stats[1], 3) if search_stats[1] else 0
        },
        "index": {
            "vectors": index.ntotal if index else 0,
            "dimension": dimension
        }
    }

if __name__ == "__main__":
    import uvicorn
    print("\n" + "="*50)
    print("ğŸš€ RAG MVPç³»ç»Ÿå¯åŠ¨ä¸­...")
    print("="*50)
    print(f"ğŸ“ æ•°æ®ç›®å½•: {DATA_DIR}")
    print(f"ğŸ“¤ ä¸Šä¼ ç›®å½•: {UPLOAD_DIR}")
    print(f"ğŸ—„ï¸ æ•°æ®åº“: {DB_PATH}")
    print("="*50 + "\n")
    
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=8000, 
        reload=False,  # ä¿®æ”¹ä¸ºFalseé¿å…è­¦å‘Š
        log_level="info"
    )