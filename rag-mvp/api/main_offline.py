#!/usr/bin/env python3
"""
RAGç¦»çº¿ç‰ˆç³»ç»Ÿ - æ— éœ€ç½‘ç»œè¿æ¥çš„ç‰ˆæœ¬
"""

from fastapi import FastAPI, UploadFile, File, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import os
import shutil
import hashlib
import json
import sqlite3
from datetime import datetime
from pathlib import Path
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="RAG Offline System",
    description="ç¦»çº¿ç‰ˆRAGç³»ç»Ÿ - æ— ç½‘ç»œä¾èµ–",
    version="1.0.0"
)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é…ç½®è·¯å¾„
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"
UPLOAD_DIR = BASE_DIR / "uploads"
DB_PATH = DATA_DIR / "rag_offline.db"

# åˆ›å»ºå¿…è¦çš„ç›®å½•
DATA_DIR.mkdir(exist_ok=True)
UPLOAD_DIR.mkdir(exist_ok=True)

# è¯·æ±‚æ¨¡å‹
class QueryRequest(BaseModel):
    query: str
    top_k: int = 5
    filter: Optional[Dict[str, Any]] = None

def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“"""
    conn = sqlite3.connect(str(DB_PATH))
    cursor = conn.cursor()
    
    # æ–‡æ¡£è¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS documents (
            id TEXT PRIMARY KEY,
            filename TEXT NOT NULL,
            file_path TEXT NOT NULL,
            upload_time TEXT,
            status TEXT,
            file_size INTEGER,
            content TEXT
        )
    ''')
    
    # æœç´¢æ—¥å¿—è¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS search_logs (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            query TEXT,
            results_count INTEGER,
            processing_time REAL,
            timestamp TEXT
        )
    ''')
    
    conn.commit()
    conn.close()
    logger.info("ç¦»çº¿æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨äº‹ä»¶"""
    print("=" * 50)
    print("ğŸš€ RAGç¦»çº¿ç‰ˆç³»ç»Ÿå¯åŠ¨ä¸­...")
    print("=" * 50)
    print(f"ğŸ“ æ•°æ®ç›®å½•: {DATA_DIR}")
    print(f"ğŸ“¤ ä¸Šä¼ ç›®å½•: {UPLOAD_DIR}")
    print(f"ğŸ—„ï¸ ç¦»çº¿æ•°æ®åº“: {DB_PATH}")
    print("=" * 50)
    
    # åˆå§‹åŒ–æ•°æ®åº“
    init_database()
    
    print("âœ… ç¦»çº¿ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
    print("âš ï¸ æ³¨æ„ï¼šå½“å‰ä¸ºç¦»çº¿æ¨¡å¼ï¼ŒåŠŸèƒ½æœ‰é™")
    print("=" * 50)

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "RAGç¦»çº¿ç‰ˆç³»ç»Ÿ",
        "version": "1.0.0",
        "mode": "offline",
        "features": [
            "æ–‡æ¡£ä¸Šä¼ ",
            "åŸºæœ¬æ–‡æœ¬æœç´¢",
            "ç®€å•é—®ç­”"
        ]
    }

@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "mode": "offline",
        "database": "connected",
        "llm": {
            "available": False,
            "provider": None,
            "model": "offline_mode"
        },
        "components": {
            "document_processor": "offline_mode",
            "vector_db": "disabled",
            "llm_generator": "disabled"
        }
    }

def read_text_file(file_path: Path) -> str:
    """è¯»å–æ–‡æœ¬æ–‡ä»¶å†…å®¹"""
    try:
        # å°è¯•ä¸åŒç¼–ç 
        encodings = ['utf-8', 'gbk', 'gb2312', 'latin1']
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    return f.read()
            except UnicodeDecodeError:
                continue
        
        # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œç”¨äºŒè¿›åˆ¶æ¨¡å¼è¯»å–å¹¶å°½å¯èƒ½è§£ç 
        with open(file_path, 'rb') as f:
            content = f.read()
            return content.decode('utf-8', errors='ignore')
            
    except Exception as e:
        logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return f"æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}"

@app.post("/api/upload")
async def upload_document(
    file: UploadFile = File(...),
    splitter_type: str = Query("char", description="åˆ‡ç‰‡ç±»å‹: char (å­—ç¬¦åˆ†å‰²)")
):
    """ä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£"""
    import time
    start_time = time.time()
    
    logger.info(f"å¼€å§‹å¤„ç†ä¸Šä¼ æ–‡ä»¶: {file.filename}")
    
    # ç”Ÿæˆæ–‡æ¡£ID
    doc_id = hashlib.md5(f"{file.filename}_{datetime.now().isoformat()}".encode()).hexdigest()[:12]
    
    # ä¿å­˜æ–‡ä»¶
    file_path = UPLOAD_DIR / f"{doc_id}_{file.filename}"
    try:
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        logger.info(f"æ–‡ä»¶ä¿å­˜æˆåŠŸ: {file_path}")
        
        # è¯»å–æ–‡ä»¶å†…å®¹ï¼ˆä»…æ”¯æŒæ–‡æœ¬æ–‡ä»¶ï¼‰
        if file.filename.lower().endswith(('.txt', '.md', '.py', '.js', '.html', '.css')):
            text_content = read_text_file(file_path)
        else:
            text_content = f"æ–‡ä»¶ç±»å‹: {file.filename.split('.')[-1].upper()}\næ–‡ä»¶å¤§å°: {len(content)} å­—èŠ‚\næ³¨æ„: ç¦»çº¿æ¨¡å¼æš‚ä¸æ”¯æŒè¯¥æ–‡ä»¶ç±»å‹çš„å†…å®¹æå–"
        
        processing_time = time.time() - start_time
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        conn = sqlite3.connect(str(DB_PATH))
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO documents (id, filename, file_path, upload_time, status, file_size, content)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (
            doc_id,
            file.filename,
            str(file_path),
            datetime.now().isoformat(),
            'completed',
            len(content),
            text_content[:10000]  # é™åˆ¶å†…å®¹é•¿åº¦
        ))
        conn.commit()
        conn.close()
        
        logger.info(f"æ–‡æ¡£å¤„ç†å®Œæˆ: {doc_id}")
        
        return {
            "success": True,
            "doc_id": doc_id,
            "filename": file.filename,
            "file_size": len(content),
            "processing_time": processing_time,
            "content_preview": text_content[:200] + "..." if len(text_content) > 200 else text_content,
            "mode": "offline",
            "note": "ç¦»çº¿æ¨¡å¼ï¼šä»…æ”¯æŒåŸºæœ¬æ–‡æœ¬æ–‡ä»¶ï¼Œæ— å‘é‡åŒ–å¤„ç†"
        }
        
    except Exception as e:
        logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥: {str(e)}")
        # æ¸…ç†æ–‡ä»¶
        if file_path.exists():
            file_path.unlink()
        raise HTTPException(status_code=500, detail=f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")

@app.post("/api/query")
async def query_documents(request: QueryRequest):
    """æŸ¥è¯¢æ–‡æ¡£ï¼ˆç¦»çº¿ç®€å•æœç´¢ï¼‰"""
    import time
    start_time = time.time()
    
    try:
        # ä»æ•°æ®åº“è·å–æ‰€æœ‰æ–‡æ¡£
        conn = sqlite3.connect(str(DB_PATH))
        cursor = conn.cursor()
        cursor.execute('SELECT id, filename, content FROM documents WHERE status = "completed"')
        documents = cursor.fetchall()
        conn.close()
        
        if not documents:
            return {
                "query": request.query,
                "results": [],
                "answer": "æš‚æ— æ–‡æ¡£å¯ä¾›æœç´¢ã€‚è¯·å…ˆä¸Šä¼ ä¸€äº›æ–‡æ¡£ã€‚",
                "processing_time": time.time() - start_time,
                "mode": "offline"
            }
        
        # ç®€å•å…³é”®è¯æœç´¢
        query_lower = request.query.lower()
        results = []
        
        for doc_id, filename, content in documents:
            if content and query_lower in content.lower():
                # æ‰¾åˆ°åŒ¹é…ä½ç½®
                match_pos = content.lower().find(query_lower)
                start_pos = max(0, match_pos - 100)
                end_pos = min(len(content), match_pos + 200)
                snippet = content[start_pos:end_pos]
                
                results.append({
                    'id': doc_id,
                    'content': snippet,
                    'metadata': {
                        'source': filename,
                        'doc_id': doc_id
                    },
                    'distance': 0.1  # æ¨¡æ‹Ÿç›¸ä¼¼åº¦
                })
        
        # é™åˆ¶ç»“æœæ•°é‡
        results = results[:request.top_k]
        
        # ç”Ÿæˆç®€å•å›ç­”
        if results:
            answer = f"åœ¨ {len(results)} ä¸ªæ–‡æ¡£ç‰‡æ®µä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼š\n\n"
            for i, result in enumerate(results[:3], 1):
                content_preview = result['content'][:150] + "..." if len(result['content']) > 150 else result['content']
                answer += f"**ç‰‡æ®µ{i}** (æ¥æº: {result['metadata']['source']}):\n{content_preview}\n\n"
            
            if len(results) > 3:
                answer += f"...ä»¥åŠå…¶ä»– {len(results)-3} ä¸ªç›¸å…³ç‰‡æ®µã€‚"
                
            answer += "\n\n> âš ï¸ ç¦»çº¿æ¨¡å¼ï¼šåŸºäºå…³é”®è¯åŒ¹é…ï¼Œå»ºè®®å¯ç”¨åœ¨çº¿æ¨¡å¼è·å¾—æ›´æ™ºèƒ½çš„å›ç­”ã€‚"
        else:
            answer = f"æœªæ‰¾åˆ°åŒ…å« '{request.query}' çš„ç›¸å…³å†…å®¹ã€‚è¯·å°è¯•å…¶ä»–å…³é”®è¯æˆ–ä¸Šä¼ æ›´å¤šæ–‡æ¡£ã€‚"
        
        processing_time = time.time() - start_time
        
        # è®°å½•æœç´¢æ—¥å¿—
        conn = sqlite3.connect(str(DB_PATH))
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO search_logs (query, results_count, processing_time, timestamp)
            VALUES (?, ?, ?, ?)
        ''', (
            request.query,
            len(results),
            processing_time,
            datetime.now().isoformat()
        ))
        conn.commit()
        conn.close()
        
        return {
            "query": request.query,
            "results": results,
            "answer": answer,
            "processing_time": processing_time,
            "mode": "offline"
        }
        
    except Exception as e:
        logger.error(f"æŸ¥è¯¢å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/documents")
async def list_documents():
    """è·å–æ–‡æ¡£åˆ—è¡¨"""
    conn = sqlite3.connect(str(DB_PATH))
    cursor = conn.cursor()
    cursor.execute('''
        SELECT id, filename, upload_time, status, file_size
        FROM documents
        ORDER BY upload_time DESC
    ''')
    rows = cursor.fetchall()
    conn.close()
    
    documents = []
    for row in rows:
        documents.append({
            "id": row[0],
            "filename": row[1],
            "upload_time": row[2],
            "status": row[3],
            "file_size": row[4],
            "mode": "offline"
        })
    
    return documents

@app.delete("/api/documents/{doc_id}")
async def delete_document(doc_id: str):
    """åˆ é™¤æ–‡æ¡£"""
    try:
        conn = sqlite3.connect(str(DB_PATH))
        cursor = conn.cursor()
        
        # è·å–æ–‡ä»¶è·¯å¾„
        cursor.execute('SELECT file_path FROM documents WHERE id = ?', (doc_id,))
        result = cursor.fetchone()
        
        if result:
            file_path = Path(result[0])
            if file_path.exists():
                file_path.unlink()
            
            # ä»æ•°æ®åº“åˆ é™¤
            cursor.execute('DELETE FROM documents WHERE id = ?', (doc_id,))
            conn.commit()
            conn.close()
            
            return {"success": True, "message": f"æ–‡æ¡£ {doc_id} å·²åˆ é™¤"}
        else:
            conn.close()
            raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
            
    except Exception as e:
        logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/statistics")
async def get_statistics():
    """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    conn = sqlite3.connect(str(DB_PATH))
    cursor = conn.cursor()
    
    cursor.execute('SELECT COUNT(*) FROM documents')
    doc_count = cursor.fetchone()[0]
    
    cursor.execute('SELECT COUNT(*) FROM search_logs')
    search_count = cursor.fetchone()[0]
    
    cursor.execute('SELECT AVG(processing_time) FROM search_logs')
    avg_processing = cursor.fetchone()[0] or 0
    
    conn.close()
    
    return {
        "documents_count": doc_count,
        "total_chunks": doc_count,  # ç¦»çº¿æ¨¡å¼ï¼šæ–‡æ¡£å³ç‰‡æ®µ
        "search_count": search_count,
        "avg_processing_time": avg_processing,
        "mode": "offline",
        "chroma_db": {
            "status": "disabled",
            "mode": "offline"
        }
    }

@app.get("/api/search-logs")
async def get_search_logs(limit: int = Query(100, description="è¿”å›è®°å½•æ•°")):
    """è·å–æœç´¢æ—¥å¿—"""
    conn = sqlite3.connect(str(DB_PATH))
    cursor = conn.cursor()
    cursor.execute('''
        SELECT query, results_count, processing_time, timestamp
        FROM search_logs
        ORDER BY timestamp DESC
        LIMIT ?
    ''', (limit,))
    rows = cursor.fetchall()
    conn.close()
    
    logs = []
    for row in rows:
        logs.append({
            "query": row[0],
            "results_count": row[1],
            "processing_time": row[2],
            "timestamp": row[3]
        })
    
    return logs

if __name__ == "__main__":
    import uvicorn
    print("ğŸ”§ å¯åŠ¨ç¦»çº¿ç‰ˆRAGç³»ç»Ÿ...")
    uvicorn.run(app, host="0.0.0.0", port=8000)