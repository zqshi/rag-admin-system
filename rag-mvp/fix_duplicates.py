#!/usr/bin/env python3
"""
ä¿®å¤RAGç³»ç»Ÿä¸­çš„é‡å¤é—®é¢˜
"""

import sqlite3
from pathlib import Path
import chromadb
from chromadb.config import Settings
import hashlib
from collections import defaultdict

def clean_duplicate_documents():
    """æ¸…ç†é‡å¤æ–‡æ¡£"""
    print("ğŸ§¹ æ¸…ç†é‡å¤æ–‡æ¡£...")
    
    # è¿æ¥æ•°æ®åº“
    db_path = Path("data/rag.db")
    conn = sqlite3.connect(str(db_path))
    cursor = conn.cursor()
    
    # æ‰¾å‡ºé‡å¤æ–‡æ¡£ï¼ˆç›¸åŒæ–‡ä»¶åï¼‰
    cursor.execute('''
        SELECT filename, COUNT(*) as count, GROUP_CONCAT(id) as ids
        FROM documents 
        GROUP BY filename 
        HAVING count > 1
        ORDER BY count DESC
    ''')
    
    duplicates = cursor.fetchall()
    
    if not duplicates:
        print("âœ… æœªå‘ç°é‡å¤æ–‡æ¡£")
        return
    
    print(f"âŒ å‘ç° {len(duplicates)} ä¸ªé‡å¤æ–‡æ¡£:")
    
    for filename, count, ids_str in duplicates:
        ids = ids_str.split(',')
        print(f"  - {filename}: {count} ä¸ªå‰¯æœ¬")
        
        # ä¿ç•™æœ€æ–°çš„ï¼Œåˆ é™¤å…¶ä»–çš„
        cursor.execute('''
            SELECT id, upload_time FROM documents 
            WHERE filename = ? 
            ORDER BY upload_time DESC
        ''', (filename,))
        
        docs = cursor.fetchall()
        keep_id = docs[0][0]  # ä¿ç•™æœ€æ–°çš„
        delete_ids = [doc[0] for doc in docs[1:]]  # åˆ é™¤å…¶ä»–çš„
        
        print(f"    ä¿ç•™: {keep_id}")
        print(f"    åˆ é™¤: {delete_ids}")
        
        # åˆ é™¤é‡å¤çš„æ–‡æ¡£è®°å½•
        for delete_id in delete_ids:
            cursor.execute('DELETE FROM documents WHERE id = ?', (delete_id,))
            print(f"    å·²åˆ é™¤æ–‡æ¡£è®°å½•: {delete_id}")
    
    conn.commit()
    conn.close()
    print("âœ… æ•°æ®åº“æ¸…ç†å®Œæˆ")

def clean_duplicate_chunks():
    """æ¸…ç†ChromaDBä¸­çš„é‡å¤ç‰‡æ®µ"""
    print("\nğŸ§¹ æ¸…ç†ChromaDBä¸­çš„é‡å¤ç‰‡æ®µ...")
    
    # è¿æ¥ChromaDB
    chroma_path = Path("chroma_db")
    client = chromadb.PersistentClient(
        path=str(chroma_path),
        settings=Settings(anonymized_telemetry=False)
    )
    
    collection = client.get_collection("rag_documents")
    
    # è·å–æ‰€æœ‰æ•°æ®
    result = collection.get(include=['documents', 'metadatas'])
    
    print(f"ğŸ“Š å½“å‰ç‰‡æ®µæ•°: {len(result['ids'])}")
    
    # æ‰¾å‡ºé‡å¤å†…å®¹
    content_to_ids = defaultdict(list)
    for i, content in enumerate(result['documents']):
        content_hash = hashlib.md5(content.encode()).hexdigest()
        content_to_ids[content_hash].append((result['ids'][i], content))
    
    # æ‰¾å‡ºéœ€è¦åˆ é™¤çš„é‡å¤é¡¹
    duplicates_to_delete = []
    unique_kept = 0
    
    for content_hash, items in content_to_ids.items():
        if len(items) > 1:
            # ä¿ç•™ç¬¬ä¸€ä¸ªï¼Œåˆ é™¤å…¶ä»–çš„
            keep_id = items[0][0]
            delete_ids = [item[0] for item in items[1:]]
            duplicates_to_delete.extend(delete_ids)
            
            print(f"å†…å®¹å“ˆå¸Œ {content_hash[:8]}...")
            print(f"  ä¿ç•™: {keep_id}")
            print(f"  åˆ é™¤: {delete_ids} (å…±{len(delete_ids)}ä¸ª)")
        
        unique_kept += 1
    
    # æ‰§è¡Œåˆ é™¤
    if duplicates_to_delete:
        print(f"\nğŸ—‘ï¸ åˆ é™¤ {len(duplicates_to_delete)} ä¸ªé‡å¤ç‰‡æ®µ...")
        collection.delete(ids=duplicates_to_delete)
        print("âœ… ChromaDBæ¸…ç†å®Œæˆ")
        
        # éªŒè¯ç»“æœ
        result_after = collection.get(include=['documents'])
        print(f"ğŸ“Š æ¸…ç†åç‰‡æ®µæ•°: {len(result_after['ids'])}")
        print(f"ğŸ“‰ åˆ é™¤äº† {len(result['ids']) - len(result_after['ids'])} ä¸ªé‡å¤ç‰‡æ®µ")
    else:
        print("âœ… æœªå‘ç°é‡å¤ç‰‡æ®µ")

def update_document_stats():
    """æ›´æ–°æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯"""
    print("\nğŸ“Š æ›´æ–°æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯...")
    
    # è¿æ¥æ•°æ®åº“
    db_path = Path("data/rag.db")
    conn = sqlite3.connect(str(db_path))
    cursor = conn.cursor()
    
    # è¿æ¥ChromaDB
    chroma_path = Path("chroma_db")
    client = chromadb.PersistentClient(
        path=str(chroma_path),
        settings=Settings(anonymized_telemetry=False)
    )
    
    collection = client.get_collection("rag_documents")
    result = collection.get(include=['metadatas'])
    
    # ç»Ÿè®¡æ¯ä¸ªæ–‡æ¡£çš„å®é™…ç‰‡æ®µæ•°
    doc_chunks = defaultdict(int)
    for metadata in result['metadatas']:
        doc_id = metadata.get('doc_id')
        if doc_id:
            doc_chunks[doc_id] += 1
    
    # æ›´æ–°æ•°æ®åº“ä¸­çš„ç»Ÿè®¡ä¿¡æ¯
    for doc_id, actual_chunk_count in doc_chunks.items():
        cursor.execute('''
            UPDATE documents 
            SET chunk_count = ? 
            WHERE id = ?
        ''', (actual_chunk_count, doc_id))
        print(f"  æ›´æ–°æ–‡æ¡£ {doc_id}: {actual_chunk_count} ç‰‡æ®µ")
    
    conn.commit()
    conn.close()
    print("âœ… ç»Ÿè®¡ä¿¡æ¯æ›´æ–°å®Œæˆ")

def main():
    print("ğŸ”§ ä¿®å¤RAGç³»ç»Ÿé‡å¤é—®é¢˜")
    print("=" * 50)
    
    try:
        # 1. æ¸…ç†é‡å¤æ–‡æ¡£è®°å½•
        clean_duplicate_documents()
        
        # 2. æ¸…ç†é‡å¤ç‰‡æ®µ
        clean_duplicate_chunks()
        
        # 3. æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        update_document_stats()
        
        print("\n" + "=" * 50)
        print("âœ… é‡å¤é—®é¢˜ä¿®å¤å®Œæˆï¼")
        print("\nå»ºè®®é‡å¯APIæœåŠ¡ä»¥ç”Ÿæ•ˆ:")
        print("  1. åœæ­¢å½“å‰æœåŠ¡ (Ctrl+C)")
        print("  2. é‡æ–°è¿è¡Œ: python api/main.py")
        
    except Exception as e:
        print(f"âŒ ä¿®å¤è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()