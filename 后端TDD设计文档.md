# RAGç³»ç»Ÿåç«¯TDDè®¾è®¡æ–‡æ¡£

**ç‰ˆæœ¬**: 1.0  
**æŠ€æœ¯æ ˆ**: Python/FastAPI + Go/Gin + PostgreSQL + Milvus  
**æ—¥æœŸ**: 2025å¹´8æœˆ12æ—¥

---

## ä¸€ã€åç«¯TDDæ¶æ„

### 1.1 æµ‹è¯•åˆ†å±‚ç­–ç•¥

```
åç«¯æµ‹è¯•é‡‘å­—å¡”
â”œâ”€â”€ å•å…ƒæµ‹è¯• (70%)
â”‚   â”œâ”€â”€ ä¸šåŠ¡é€»è¾‘å±‚
â”‚   â”œâ”€â”€ æ•°æ®è®¿é—®å±‚
â”‚   â”œâ”€â”€ å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ ç®—æ³•æ¨¡å—
â”œâ”€â”€ é›†æˆæµ‹è¯• (20%)
â”‚   â”œâ”€â”€ APIç«¯ç‚¹æµ‹è¯•
â”‚   â”œâ”€â”€ æ•°æ®åº“äº‹åŠ¡æµ‹è¯•
â”‚   â”œâ”€â”€ æ¶ˆæ¯é˜Ÿåˆ—æµ‹è¯•
â”‚   â””â”€â”€ ç¼“å­˜å±‚æµ‹è¯•
â”œâ”€â”€ å¥‘çº¦æµ‹è¯• (5%)
â”‚   â”œâ”€â”€ å‰åç«¯å¥‘çº¦
â”‚   â””â”€â”€ å¾®æœåŠ¡é—´å¥‘çº¦
â””â”€â”€ ç³»ç»Ÿæµ‹è¯• (5%)
    â”œâ”€â”€ ç«¯åˆ°ç«¯æµç¨‹
    â”œâ”€â”€ æ€§èƒ½æµ‹è¯•
    â””â”€â”€ å‹åŠ›æµ‹è¯•
```

### 1.2 æµ‹è¯•ç¯å¢ƒæ¶æ„

```yaml
# docker-compose.test.yml
version: '3.8'

services:
  test-db:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: rag_test
      POSTGRES_USER: test
      POSTGRES_PASSWORD: test
    ports:
      - "5433:5432"
    tmpfs:
      - /var/lib/postgresql/data
      
  test-redis:
    image: redis:7-alpine
    command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru
    ports:
      - "6380:6379"
      
  test-milvus:
    image: milvusdb/milvus:v2.3.0
    environment:
      ETCD_USE_EMBED: true
      COMMON_STORAGETYPE: local
    ports:
      - "19531:19530"
      
  test-kafka:
    image: confluentinc/cp-kafka:7.4.0
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9093
    ports:
      - "9093:9092"
```

---

## äºŒã€PythonæœåŠ¡TDDå®ç°

### 2.1 æ–‡æ¡£å¤„ç†æœåŠ¡

#### æ­¥éª¤1ï¼šç¼–å†™å¤±è´¥æµ‹è¯• ğŸ”´

```python
# tests/test_document_processor.py
import pytest
from unittest.mock import Mock, patch, AsyncMock
from datetime import datetime
from io import BytesIO

from app.services.document_processor import DocumentProcessor
from app.models.document import Document, DocumentStatus, ChunkingStrategy
from app.exceptions import ParseError, ValidationError


class TestDocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨æµ‹è¯•å¥—ä»¶"""
    
    @pytest.fixture
    def processor(self):
        """åˆ›å»ºå¤„ç†å™¨å®ä¾‹"""
        return DocumentProcessor()
    
    @pytest.fixture
    def valid_pdf(self):
        """åˆ›å»ºæœ‰æ•ˆçš„PDFæ–‡ä»¶"""
        content = b"%PDF-1.4\n%fake pdf content"
        return BytesIO(content)
    
    @pytest.fixture
    def mock_llm_service(self):
        """Mock LLMæœåŠ¡"""
        with patch('app.services.llm_service.LLMService') as mock:
            mock_instance = mock.return_value
            mock_instance.generate_embedding = AsyncMock(
                return_value=[[0.1] * 1536]
            )
            yield mock_instance
    
    # ============ æ–‡æ¡£è§£ææµ‹è¯• ============
    
    @pytest.mark.asyncio
    async def test_should_parse_valid_pdf_document(self, processor, valid_pdf):
        """åº”è¯¥æˆåŠŸè§£ææœ‰æ•ˆçš„PDFæ–‡æ¡£"""
        # Given
        document = Document(
            id="doc_123",
            name="test.pdf",
            content=valid_pdf.read(),
            type="application/pdf",
            size=1024
        )
        
        # When
        result = await processor.parse(document)
        
        # Then
        assert result.status == DocumentStatus.PARSED
        assert result.text_content is not None
        assert len(result.text_content) > 0
        assert result.metadata['pages'] > 0
        assert result.parse_time < 5.0  # è§£ææ—¶é—´å°äº5ç§’
    
    @pytest.mark.asyncio
    async def test_should_reject_corrupted_file(self, processor):
        """åº”è¯¥æ‹’ç»æŸåçš„æ–‡ä»¶"""
        # Given
        document = Document(
            id="doc_124",
            name="corrupted.pdf",
            content=b"invalid pdf content",
            type="application/pdf"
        )
        
        # When/Then
        with pytest.raises(ParseError) as exc_info:
            await processor.parse(document)
        
        assert "æ— æ³•è§£æPDFæ–‡ä»¶" in str(exc_info.value)
        assert exc_info.value.document_id == "doc_124"
    
    @pytest.mark.asyncio
    async def test_should_handle_multiple_file_formats(self, processor):
        """åº”è¯¥å¤„ç†å¤šç§æ–‡ä»¶æ ¼å¼"""
        formats = [
            ("test.pdf", "application/pdf", b"%PDF-1.4"),
            ("test.docx", "application/vnd.openxmlformats", b"PK\x03\x04"),
            ("test.txt", "text/plain", b"Plain text content"),
            ("test.md", "text/markdown", b"# Markdown content"),
            ("test.html", "text/html", b"<html><body>HTML</body></html>")
        ]
        
        for name, mime_type, content in formats:
            document = Document(
                id=f"doc_{name}",
                name=name,
                content=content,
                type=mime_type
            )
            
            result = await processor.parse(document)
            
            assert result.status == DocumentStatus.PARSED
            assert result.text_content is not None
    
    # ============ æ–‡æ¡£åˆ†ç‰‡æµ‹è¯• ============
    
    @pytest.mark.asyncio
    async def test_should_chunk_document_with_overlap(self, processor):
        """åº”è¯¥ä½¿ç”¨é‡å ç­–ç•¥åˆ†ç‰‡æ–‡æ¡£"""
        # Given
        text = " ".join([f"Sentence {i}." for i in range(100)])
        document = Document(
            id="doc_125",
            text_content=text,
            status=DocumentStatus.PARSED
        )
        
        strategy = ChunkingStrategy(
            method="sliding_window",
            chunk_size=100,
            overlap=20
        )
        
        # When
        chunks = await processor.chunk(document, strategy)
        
        # Then
        assert len(chunks) > 1
        assert all(len(chunk.content) <= 100 for chunk in chunks)
        
        # éªŒè¯é‡å 
        for i in range(len(chunks) - 1):
            current_end = chunks[i].content[-20:]
            next_start = chunks[i + 1].content[:20]
            assert current_end in chunks[i + 1].content
    
    @pytest.mark.asyncio
    async def test_should_preserve_semantic_boundaries(self, processor):
        """åº”è¯¥ä¿æŒè¯­ä¹‰è¾¹ç•Œå®Œæ•´"""
        # Given
        text = """
        ç¬¬ä¸€ç« ï¼šä»‹ç»
        è¿™æ˜¯ç¬¬ä¸€ç« çš„å†…å®¹ï¼ŒåŒ…å«é‡è¦ä¿¡æ¯ã€‚
        
        ç¬¬äºŒç« ï¼šæ–¹æ³•
        è¿™æ˜¯ç¬¬äºŒç« çš„å†…å®¹ï¼Œæè¿°å…·ä½“æ–¹æ³•ã€‚
        
        ç¬¬ä¸‰ç« ï¼šç»“è®º
        è¿™æ˜¯ç¬¬ä¸‰ç« çš„å†…å®¹ï¼Œæ€»ç»“è¦ç‚¹ã€‚
        """
        
        document = Document(id="doc_126", text_content=text)
        strategy = ChunkingStrategy(method="semantic", chunk_size=150)
        
        # When
        chunks = await processor.chunk(document, strategy)
        
        # Then
        # æ¯ä¸ªç« èŠ‚åº”è¯¥åœ¨ç‹¬ç«‹çš„chunkä¸­
        assert any("ç¬¬ä¸€ç« " in chunk.content for chunk in chunks)
        assert any("ç¬¬äºŒç« " in chunk.content for chunk in chunks)
        assert any("ç¬¬ä¸‰ç« " in chunk.content for chunk in chunks)
        
        # ç« èŠ‚ä¸åº”è¯¥è¢«æ‹†åˆ†
        for chunk in chunks:
            if "ç¬¬ä¸€ç« " in chunk.content:
                assert "ç¬¬ä¸€ç« çš„å†…å®¹" in chunk.content
    
    # ============ å‘é‡åŒ–æµ‹è¯• ============
    
    @pytest.mark.asyncio
    async def test_should_generate_embeddings_for_chunks(
        self, processor, mock_llm_service
    ):
        """åº”è¯¥ä¸ºæ–‡æ¡£ç‰‡æ®µç”Ÿæˆå‘é‡"""
        # Given
        chunks = [
            {"id": "chunk_1", "content": "First chunk"},
            {"id": "chunk_2", "content": "Second chunk"},
            {"id": "chunk_3", "content": "Third chunk"}
        ]
        
        # When
        embeddings = await processor.generate_embeddings(chunks)
        
        # Then
        assert len(embeddings) == 3
        assert all(len(emb.vector) == 1536 for emb in embeddings)
        assert all(emb.model == "text-embedding-ada-002" for emb in embeddings)
        
        # éªŒè¯è°ƒç”¨æ¬¡æ•°
        assert mock_llm_service.generate_embedding.call_count == 3
    
    @pytest.mark.asyncio
    async def test_should_batch_embedding_requests(self, processor, mock_llm_service):
        """åº”è¯¥æ‰¹é‡å¤„ç†å‘é‡åŒ–è¯·æ±‚"""
        # Given
        chunks = [{"id": f"chunk_{i}", "content": f"Content {i}"} 
                 for i in range(100)]
        
        # When
        embeddings = await processor.generate_embeddings(
            chunks, 
            batch_size=10
        )
        
        # Then
        assert len(embeddings) == 100
        # åº”è¯¥è°ƒç”¨10æ¬¡ï¼ˆ100ä¸ªchunksï¼Œæ¯æ‰¹10ä¸ªï¼‰
        assert mock_llm_service.generate_embedding.call_count == 10
    
    # ============ è´¨é‡è¯„ä¼°æµ‹è¯• ============
    
    @pytest.mark.asyncio
    async def test_should_evaluate_chunk_quality(self, processor):
        """åº”è¯¥è¯„ä¼°æ–‡æ¡£ç‰‡æ®µè´¨é‡"""
        # Given
        high_quality_chunk = {
            "content": "This is a well-structured paragraph with clear information about the topic. It contains relevant keywords and maintains coherent flow.",
            "tokens": 25
        }
        
        low_quality_chunk = {
            "content": "This... um... like... you know...",
            "tokens": 6
        }
        
        # When
        high_score = await processor.evaluate_quality(high_quality_chunk)
        low_score = await processor.evaluate_quality(low_quality_chunk)
        
        # Then
        assert high_score > 0.7
        assert low_score < 0.3
        assert 0 <= high_score <= 1
        assert 0 <= low_score <= 1
```

#### æ­¥éª¤2ï¼šå®ç°æœ€å°ä»£ç  ğŸŸ¢

```python
# app/services/document_processor.py
from typing import List, Dict, Any, Optional
import asyncio
from datetime import datetime
import hashlib

from app.models.document import Document, DocumentStatus, ChunkingStrategy
from app.services.llm_service import LLMService
from app.services.parser_factory import ParserFactory
from app.exceptions import ParseError, ValidationError


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†æ ¸å¿ƒæœåŠ¡"""
    
    def __init__(self):
        self.llm_service = LLMService()
        self.parser_factory = ParserFactory()
    
    async def parse(self, document: Document) -> Document:
        """è§£ææ–‡æ¡£å†…å®¹"""
        start_time = datetime.now()
        
        try:
            # è·å–å¯¹åº”çš„è§£æå™¨
            parser = self.parser_factory.get_parser(document.type)
            
            # è§£ææ–‡æ¡£
            text_content = await parser.parse(document.content)
            metadata = await parser.extract_metadata(document.content)
            
            # æ›´æ–°æ–‡æ¡£çŠ¶æ€
            document.text_content = text_content
            document.metadata = metadata
            document.status = DocumentStatus.PARSED
            document.parse_time = (datetime.now() - start_time).total_seconds()
            
            return document
            
        except Exception as e:
            raise ParseError(
                f"æ— æ³•è§£æ{document.type}æ–‡ä»¶",
                document_id=document.id,
                original_error=str(e)
            )
    
    async def chunk(
        self, 
        document: Document, 
        strategy: ChunkingStrategy
    ) -> List[Dict[str, Any]]:
        """æ–‡æ¡£åˆ†ç‰‡"""
        if strategy.method == "sliding_window":
            return self._sliding_window_chunk(
                document.text_content,
                strategy.chunk_size,
                strategy.overlap
            )
        elif strategy.method == "semantic":
            return self._semantic_chunk(
                document.text_content,
                strategy.chunk_size
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†ç‰‡ç­–ç•¥: {strategy.method}")
    
    def _sliding_window_chunk(
        self, 
        text: str, 
        chunk_size: int, 
        overlap: int
    ) -> List[Dict[str, Any]]:
        """æ»‘åŠ¨çª—å£åˆ†ç‰‡"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = min(start + chunk_size, len(text))
            chunk_content = text[start:end]
            
            chunks.append({
                "content": chunk_content,
                "start": start,
                "end": end,
                "id": hashlib.md5(chunk_content.encode()).hexdigest()
            })
            
            start += chunk_size - overlap
            
        return chunks
    
    def _semantic_chunk(
        self, 
        text: str, 
        chunk_size: int
    ) -> List[Dict[str, Any]]:
        """è¯­ä¹‰åˆ†ç‰‡"""
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            if len(current_chunk) + len(para) <= chunk_size:
                current_chunk += para + "\n\n"
            else:
                if current_chunk:
                    chunks.append({
                        "content": current_chunk.strip(),
                        "id": hashlib.md5(current_chunk.encode()).hexdigest()
                    })
                current_chunk = para + "\n\n"
        
        if current_chunk:
            chunks.append({
                "content": current_chunk.strip(),
                "id": hashlib.md5(current_chunk.encode()).hexdigest()
            })
        
        return chunks
    
    async def generate_embeddings(
        self, 
        chunks: List[Dict[str, Any]], 
        batch_size: int = 10
    ) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå‘é‡åµŒå…¥"""
        embeddings = []
        
        # æ‰¹é‡å¤„ç†
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i + batch_size]
            batch_texts = [chunk["content"] for chunk in batch]
            
            # å¹¶å‘ç”Ÿæˆå‘é‡
            vectors = await self.llm_service.generate_embedding(batch_texts)
            
            for chunk, vector in zip(batch, vectors):
                embeddings.append({
                    "chunk_id": chunk["id"],
                    "vector": vector,
                    "model": "text-embedding-ada-002"
                })
        
        return embeddings
    
    async def evaluate_quality(self, chunk: Dict[str, Any]) -> float:
        """è¯„ä¼°ç‰‡æ®µè´¨é‡"""
        content = chunk["content"]
        tokens = chunk.get("tokens", len(content.split()))
        
        # è´¨é‡è¯„åˆ†è§„åˆ™
        score = 0.0
        
        # é•¿åº¦è¯„åˆ†
        if tokens >= 20:
            score += 0.3
        elif tokens >= 10:
            score += 0.2
        elif tokens >= 5:
            score += 0.1
        
        # å†…å®¹è´¨é‡è¯„åˆ†
        if not any(filler in content.lower() 
                  for filler in ["um", "uh", "like", "you know"]):
            score += 0.3
        
        # ç»“æ„è¯„åˆ†
        if "." in content and len(content.split(".")) > 1:
            score += 0.2
        
        # ä¿¡æ¯å¯†åº¦è¯„åˆ†
        unique_words = len(set(content.lower().split()))
        total_words = len(content.split())
        if total_words > 0:
            diversity = unique_words / total_words
            score += diversity * 0.2
        
        return min(score, 1.0)
```

#### æ­¥éª¤3ï¼šé‡æ„ä¼˜åŒ– ğŸ”„

```python
# app/services/document_processor_optimized.py
from typing import List, Dict, Any, Protocol
import asyncio
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache
import numpy as np

from app.models.document import Document
from app.services.quality_evaluator import QualityEvaluator
from app.utils.metrics import track_performance


class ChunkingStrategy(Protocol):
    """åˆ†ç‰‡ç­–ç•¥åè®®"""
    async def chunk(self, text: str) -> List[Dict[str, Any]]:
        ...


class OptimizedDocumentProcessor:
    """ä¼˜åŒ–çš„æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.quality_evaluator = QualityEvaluator()
        self._cache = {}
    
    @track_performance
    async def process_batch(
        self, 
        documents: List[Document]
    ) -> List[Document]:
        """æ‰¹é‡å¤„ç†æ–‡æ¡£"""
        # å¹¶å‘å¤„ç†
        tasks = [self.process_single(doc) for doc in documents]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        processed = []
        failed = []
        
        for doc, result in zip(documents, results):
            if isinstance(result, Exception):
                failed.append((doc, result))
            else:
                processed.append(result)
        
        # é‡è¯•å¤±è´¥çš„æ–‡æ¡£
        if failed and self.config.get('retry_enabled', True):
            retry_results = await self._retry_failed(failed)
            processed.extend(retry_results)
        
        return processed
    
    async def process_single(self, document: Document) -> Document:
        """å¤„ç†å•ä¸ªæ–‡æ¡£"""
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(document)
        if cache_key in self._cache:
            return self._cache[cache_key]
        
        # å¤„ç†æµæ°´çº¿
        pipeline = [
            self._validate,
            self._parse,
            self._chunk,
            self._generate_embeddings,
            self._evaluate_quality,
            self._index
        ]
        
        result = document
        for step in pipeline:
            result = await step(result)
        
        # ç¼“å­˜ç»“æœ
        self._cache[cache_key] = result
        
        return result
    
    @lru_cache(maxsize=128)
    def _get_cache_key(self, document: Document) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.sha256(
            f"{document.id}:{document.content_hash}".encode()
        ).hexdigest()
    
    async def _retry_failed(
        self, 
        failed: List[Tuple[Document, Exception]]
    ) -> List[Document]:
        """é‡è¯•å¤±è´¥çš„æ–‡æ¡£"""
        retry_results = []
        max_retries = self.config.get('max_retries', 3)
        
        for doc, error in failed:
            for attempt in range(max_retries):
                try:
                    await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    result = await self.process_single(doc)
                    retry_results.append(result)
                    break
                except Exception as e:
                    if attempt == max_retries - 1:
                        logger.error(f"æ–‡æ¡£ {doc.id} å¤„ç†å¤±è´¥: {e}")
        
        return retry_results
```

### 2.2 æ£€ç´¢æœåŠ¡æµ‹è¯•ï¼ˆGoï¼‰

```go
// search_service_test.go
package services

import (
    "context"
    "testing"
    "time"
    
    "github.com/stretchr/testify/assert"
    "github.com/stretchr/testify/mock"
    "github.com/stretchr/testify/suite"
)

type SearchServiceTestSuite struct {
    suite.Suite
    service      *SearchService
    mockVectorDB *MockVectorDB
    mockCache    *MockCache
    mockReranker *MockReranker
}

func (suite *SearchServiceTestSuite) SetupTest() {
    suite.mockVectorDB = new(MockVectorDB)
    suite.mockCache = new(MockCache)
    suite.mockReranker = new(MockReranker)
    
    suite.service = NewSearchService(
        suite.mockVectorDB,
        suite.mockCache,
        suite.mockReranker,
    )
}

// ============ å‘é‡æ£€ç´¢æµ‹è¯• ============

func (suite *SearchServiceTestSuite) TestVectorSearch_Success() {
    // Given
    ctx := context.Background()
    query := Query{
        Text:  "å¦‚ä½•é…ç½®RAGç³»ç»Ÿ",
        TopK:  10,
        Threshold: 0.7,
    }
    
    expectedDocs := []Document{
        {ID: "doc1", Score: 0.95, Content: "RAGé…ç½®æŒ‡å—"},
        {ID: "doc2", Score: 0.85, Content: "ç³»ç»Ÿè®¾ç½®è¯´æ˜"},
    }
    
    suite.mockVectorDB.On("Search", ctx, query).Return(expectedDocs, nil)
    
    // When
    results, err := suite.service.VectorSearch(ctx, query)
    
    // Then
    assert.NoError(suite.T(), err)
    assert.Len(suite.T(), results, 2)
    assert.Equal(suite.T(), expectedDocs[0].ID, results[0].ID)
    suite.mockVectorDB.AssertExpectations(suite.T())
}

func (suite *SearchServiceTestSuite) TestVectorSearch_WithCache() {
    // Given
    ctx := context.Background()
    query := Query{Text: "cached query"}
    cacheKey := suite.service.getCacheKey(query)
    
    cachedResults := []Document{
        {ID: "cached1", Score: 0.9},
    }
    
    suite.mockCache.On("Get", cacheKey).Return(cachedResults, nil)
    
    // When
    results, err := suite.service.VectorSearch(ctx, query)
    
    // Then
    assert.NoError(suite.T(), err)
    assert.Equal(suite.T(), cachedResults, results)
    
    // ä¸åº”è¯¥è°ƒç”¨VectorDB
    suite.mockVectorDB.AssertNotCalled(suite.T(), "Search")
}

// ============ æ··åˆæ£€ç´¢æµ‹è¯• ============

func (suite *SearchServiceTestSuite) TestHybridSearch_CombinesResults() {
    // Given
    ctx := context.Background()
    query := HybridQuery{
        Text: "test query",
        VectorWeight: 0.6,
        KeywordWeight: 0.4,
    }
    
    vectorResults := []Document{
        {ID: "vec1", Score: 0.9},
        {ID: "vec2", Score: 0.8},
        {ID: "common", Score: 0.7},
    }
    
    keywordResults := []Document{
        {ID: "key1", Score: 0.85},
        {ID: "key2", Score: 0.75},
        {ID: "common", Score: 0.65},
    }
    
    suite.mockVectorDB.On("Search", mock.Anything, mock.Anything).
        Return(vectorResults, nil)
    
    suite.mockKeywordDB.On("Search", mock.Anything, mock.Anything).
        Return(keywordResults, nil)
    
    // When
    results, err := suite.service.HybridSearch(ctx, query)
    
    // Then
    assert.NoError(suite.T(), err)
    
    // éªŒè¯ç»“æœèåˆ
    assert.Len(suite.T(), results, 5) // å»é‡åçš„ç»“æœ
    
    // éªŒè¯åˆ†æ•°èåˆï¼ˆRRFç®—æ³•ï¼‰
    commonDoc := findDocByID(results, "common")
    assert.NotNil(suite.T(), commonDoc)
    
    // commonæ–‡æ¡£åº”è¯¥æœ‰æ›´é«˜çš„ç»¼åˆåˆ†æ•°
    assert.Greater(suite.T(), commonDoc.Score, 0.7)
}

func (suite *SearchServiceTestSuite) TestHybridSearch_Timeout() {
    // Given
    ctx, cancel := context.WithTimeout(context.Background(), 100*time.Millisecond)
    defer cancel()
    
    query := HybridQuery{Text: "test"}
    
    // æ¨¡æ‹Ÿæ…¢æŸ¥è¯¢
    suite.mockVectorDB.On("Search", mock.Anything, mock.Anything).
        Run(func(args mock.Arguments) {
            time.Sleep(200 * time.Millisecond)
        }).
        Return(nil, context.DeadlineExceeded)
    
    // When
    _, err := suite.service.HybridSearch(ctx, query)
    
    // Then
    assert.Error(suite.T(), err)
    assert.Contains(suite.T(), err.Error(), "timeout")
}

// ============ é‡æ’åºæµ‹è¯• ============

func (suite *SearchServiceTestSuite) TestRerank_ImproveRelevance() {
    // Given
    candidates := []Document{
        {ID: "doc1", Score: 0.8, Content: "ç›¸å…³å†…å®¹"},
        {ID: "doc2", Score: 0.9, Content: "ä¸å¤ªç›¸å…³"},
        {ID: "doc3", Score: 0.7, Content: "éå¸¸ç›¸å…³"},
    }
    
    query := "æŸ¥æ‰¾ç›¸å…³å†…å®¹"
    
    // æ¨¡æ‹Ÿé‡æ’åºåçš„ç»“æœ
    rerankedScores := map[string]float64{
        "doc1": 0.95,  // æå‡
        "doc2": 0.6,   // é™ä½
        "doc3": 0.98,  // å¤§å¹…æå‡
    }
    
    suite.mockReranker.On("Rerank", candidates, query).
        Return(func(docs []Document, q string) []Document {
            for i := range docs {
                docs[i].RerankScore = rerankedScores[docs[i].ID]
            }
            // æŒ‰æ–°åˆ†æ•°æ’åº
            sort.Slice(docs, func(i, j int) bool {
                return docs[i].RerankScore > docs[j].RerankScore
            })
            return docs
        }, nil)
    
    // When
    results := suite.service.Rerank(candidates, query)
    
    // Then
    assert.Equal(suite.T(), "doc3", results[0].ID) // æœ€ç›¸å…³çš„æ’ç¬¬ä¸€
    assert.Equal(suite.T(), "doc1", results[1].ID)
    assert.Equal(suite.T(), "doc2", results[2].ID) // ä¸ç›¸å…³çš„æ’æœ€å
}

// ============ å¹¶å‘æµ‹è¯• ============

func (suite *SearchServiceTestSuite) TestConcurrentSearches() {
    // Given
    numRequests := 100
    ctx := context.Background()
    
    suite.mockVectorDB.On("Search", mock.Anything, mock.Anything).
        Return([]Document{{ID: "test"}}, nil)
    
    // When
    results := make(chan error, numRequests)
    
    for i := 0; i < numRequests; i++ {
        go func(id int) {
            query := Query{Text: fmt.Sprintf("query_%d", id)}
            _, err := suite.service.VectorSearch(ctx, query)
            results <- err
        }(i)
    }
    
    // Then
    for i := 0; i < numRequests; i++ {
        err := <-results
        assert.NoError(suite.T(), err)
    }
    
    // éªŒè¯è°ƒç”¨æ¬¡æ•°
    suite.mockVectorDB.AssertNumberOfCalls(suite.T(), "Search", numRequests)
}

// ============ æ€§èƒ½åŸºå‡†æµ‹è¯• ============

func BenchmarkVectorSearch(b *testing.B) {
    service := setupBenchmarkService()
    ctx := context.Background()
    query := Query{Text: "benchmark query", TopK: 10}
    
    b.ResetTimer()
    b.RunParallel(func(pb *testing.PB) {
        for pb.Next() {
            _, err := service.VectorSearch(ctx, query)
            if err != nil {
                b.Fatal(err)
            }
        }
    })
}

func BenchmarkHybridSearch(b *testing.B) {
    service := setupBenchmarkService()
    ctx := context.Background()
    query := HybridQuery{
        Text: "benchmark query",
        VectorWeight: 0.6,
        KeywordWeight: 0.4,
    }
    
    b.ResetTimer()
    for i := 0; i < b.N; i++ {
        _, err := service.HybridSearch(ctx, query)
        if err != nil {
            b.Fatal(err)
        }
    }
}

func TestSearchServiceSuite(t *testing.T) {
    suite.Run(t, new(SearchServiceTestSuite))
}
```

---

## ä¸‰ã€æ•°æ®è®¿é—®å±‚TDD

### 3.1 Repositoryå±‚æµ‹è¯•

```python
# tests/test_document_repository.py
import pytest
from datetime import datetime, timedelta
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from app.repositories.document_repository import DocumentRepository
from app.models.database import Base, DocumentModel
from app.exceptions import NotFoundError, DuplicateError


class TestDocumentRepository:
    """æ–‡æ¡£ä»“åº“æµ‹è¯•"""
    
    @pytest.fixture(scope="function")
    def db_session(self):
        """åˆ›å»ºæµ‹è¯•æ•°æ®åº“ä¼šè¯"""
        # ä½¿ç”¨å†…å­˜æ•°æ®åº“
        engine = create_engine("sqlite:///:memory:")
        Base.metadata.create_all(engine)
        
        SessionLocal = sessionmaker(bind=engine)
        session = SessionLocal()
        
        yield session
        
        session.close()
        Base.metadata.drop_all(engine)
    
    @pytest.fixture
    def repository(self, db_session):
        """åˆ›å»ºä»“åº“å®ä¾‹"""
        return DocumentRepository(db_session)
    
    @pytest.fixture
    def sample_document(self):
        """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£"""
        return DocumentModel(
            id="doc_001",
            name="test.pdf",
            type="application/pdf",
            size=1024,
            status="pending",
            created_at=datetime.utcnow()
        )
    
    # ============ CRUDæµ‹è¯• ============
    
    def test_should_create_document(self, repository, sample_document):
        """åº”è¯¥åˆ›å»ºæ–‡æ¡£"""
        # When
        created = repository.create(sample_document)
        
        # Then
        assert created.id == "doc_001"
        assert repository.count() == 1
    
    def test_should_prevent_duplicate_documents(self, repository, sample_document):
        """åº”è¯¥é˜²æ­¢é‡å¤æ–‡æ¡£"""
        # Given
        repository.create(sample_document)
        
        # When/Then
        duplicate = DocumentModel(id="doc_001", name="duplicate.pdf")
        with pytest.raises(DuplicateError):
            repository.create(duplicate)
    
    def test_should_find_document_by_id(self, repository, sample_document):
        """åº”è¯¥é€šè¿‡IDæŸ¥æ‰¾æ–‡æ¡£"""
        # Given
        repository.create(sample_document)
        
        # When
        found = repository.find_by_id("doc_001")
        
        # Then
        assert found is not None
        assert found.name == "test.pdf"
    
    def test_should_raise_when_document_not_found(self, repository):
        """æ‰¾ä¸åˆ°æ–‡æ¡£æ—¶åº”è¯¥æŠ›å‡ºå¼‚å¸¸"""
        # When/Then
        with pytest.raises(NotFoundError) as exc_info:
            repository.find_by_id("non_existent")
        
        assert "Document non_existent not found" in str(exc_info.value)
    
    def test_should_update_document(self, repository, sample_document):
        """åº”è¯¥æ›´æ–°æ–‡æ¡£"""
        # Given
        repository.create(sample_document)
        
        # When
        updates = {"status": "processed", "chunks_count": 10}
        updated = repository.update("doc_001", updates)
        
        # Then
        assert updated.status == "processed"
        assert updated.chunks_count == 10
        assert updated.updated_at > sample_document.created_at
    
    def test_should_delete_document(self, repository, sample_document):
        """åº”è¯¥åˆ é™¤æ–‡æ¡£"""
        # Given
        repository.create(sample_document)
        assert repository.count() == 1
        
        # When
        repository.delete("doc_001")
        
        # Then
        assert repository.count() == 0
        with pytest.raises(NotFoundError):
            repository.find_by_id("doc_001")
    
    # ============ æŸ¥è¯¢æµ‹è¯• ============
    
    def test_should_filter_documents_by_status(self, repository):
        """åº”è¯¥æŒ‰çŠ¶æ€è¿‡æ»¤æ–‡æ¡£"""
        # Given
        docs = [
            DocumentModel(id=f"doc_{i}", status=status)
            for i, status in enumerate(["pending", "processed", "pending", "failed"])
        ]
        for doc in docs:
            repository.create(doc)
        
        # When
        pending = repository.find_by_status("pending")
        processed = repository.find_by_status("processed")
        
        # Then
        assert len(pending) == 2
        assert len(processed) == 1
        assert all(d.status == "pending" for d in pending)
    
    def test_should_paginate_results(self, repository):
        """åº”è¯¥åˆ†é¡µè¿”å›ç»“æœ"""
        # Given
        for i in range(25):
            repository.create(DocumentModel(id=f"doc_{i:03d}", name=f"file_{i}.pdf"))
        
        # When
        page1 = repository.find_all(page=1, per_page=10)
        page2 = repository.find_all(page=2, per_page=10)
        page3 = repository.find_all(page=3, per_page=10)
        
        # Then
        assert len(page1.items) == 10
        assert len(page2.items) == 10
        assert len(page3.items) == 5
        assert page1.total == 25
        assert page1.pages == 3
    
    def test_should_search_documents(self, repository):
        """åº”è¯¥æœç´¢æ–‡æ¡£"""
        # Given
        docs = [
            DocumentModel(id="1", name="Pythonæ•™ç¨‹.pdf"),
            DocumentModel(id="2", name="JavaæŒ‡å—.docx"),
            DocumentModel(id="3", name="Pythonå…¥é—¨.txt"),
        ]
        for doc in docs:
            repository.create(doc)
        
        # When
        results = repository.search("Python")
        
        # Then
        assert len(results) == 2
        assert all("Python" in r.name for r in results)
    
    # ============ äº‹åŠ¡æµ‹è¯• ============
    
    def test_should_rollback_on_error(self, repository):
        """é”™è¯¯æ—¶åº”è¯¥å›æ»šäº‹åŠ¡"""
        # Given
        initial_count = repository.count()
        
        # When
        try:
            with repository.transaction():
                repository.create(DocumentModel(id="tx_1", name="test1.pdf"))
                repository.create(DocumentModel(id="tx_2", name="test2.pdf"))
                # æ•…æ„è§¦å‘é”™è¯¯
                raise ValueError("Simulated error")
        except ValueError:
            pass
        
        # Then
        assert repository.count() == initial_count  # äº‹åŠ¡å›æ»š
    
    def test_should_commit_successful_transaction(self, repository):
        """æˆåŠŸæ—¶åº”è¯¥æäº¤äº‹åŠ¡"""
        # Given
        initial_count = repository.count()
        
        # When
        with repository.transaction():
            repository.create(DocumentModel(id="tx_1", name="test1.pdf"))
            repository.create(DocumentModel(id="tx_2", name="test2.pdf"))
        
        # Then
        assert repository.count() == initial_count + 2
    
    # ============ æ‰¹é‡æ“ä½œæµ‹è¯• ============
    
    def test_should_bulk_insert(self, repository):
        """åº”è¯¥æ‰¹é‡æ’å…¥"""
        # Given
        documents = [
            DocumentModel(id=f"bulk_{i}", name=f"file_{i}.pdf")
            for i in range(100)
        ]
        
        # When
        start_time = datetime.now()
        repository.bulk_insert(documents)
        duration = (datetime.now() - start_time).total_seconds()
        
        # Then
        assert repository.count() == 100
        assert duration < 1.0  # æ‰¹é‡æ’å…¥åº”è¯¥å¾ˆå¿«
    
    def test_should_bulk_update(self, repository):
        """åº”è¯¥æ‰¹é‡æ›´æ–°"""
        # Given
        docs = [DocumentModel(id=f"doc_{i}", status="pending") for i in range(10)]
        repository.bulk_insert(docs)
        
        # When
        ids = [f"doc_{i}" for i in range(5)]
        repository.bulk_update(ids, {"status": "processed"})
        
        # Then
        processed = repository.find_by_status("processed")
        pending = repository.find_by_status("pending")
        
        assert len(processed) == 5
        assert len(pending) == 5
```

### 3.2 ç¼“å­˜å±‚æµ‹è¯•

```python
# tests/test_cache_service.py
import pytest
import asyncio
from unittest.mock import Mock, patch
import redis
import json

from app.services.cache_service import CacheService, CacheKey


class TestCacheService:
    """ç¼“å­˜æœåŠ¡æµ‹è¯•"""
    
    @pytest.fixture
    def redis_mock(self):
        """Mock Rediså®¢æˆ·ç«¯"""
        with patch('redis.asyncio.Redis') as mock:
            yield mock.return_value
    
    @pytest.fixture
    def cache_service(self, redis_mock):
        """åˆ›å»ºç¼“å­˜æœåŠ¡"""
        return CacheService(redis_mock)
    
    # ============ åŸºç¡€æ“ä½œæµ‹è¯• ============
    
    @pytest.mark.asyncio
    async def test_should_set_and_get_value(self, cache_service, redis_mock):
        """åº”è¯¥è®¾ç½®å’Œè·å–å€¼"""
        # Given
        key = "test_key"
        value = {"data": "test_value"}
        ttl = 300
        
        redis_mock.set = Mock(return_value=True)
        redis_mock.get = Mock(return_value=json.dumps(value))
        
        # When
        await cache_service.set(key, value, ttl)
        result = await cache_service.get(key)
        
        # Then
        assert result == value
        redis_mock.set.assert_called_once()
        redis_mock.get.assert_called_once_with(key)
    
    @pytest.mark.asyncio
    async def test_should_return_none_for_missing_key(self, cache_service, redis_mock):
        """ç¼ºå¤±çš„é”®åº”è¯¥è¿”å›None"""
        # Given
        redis_mock.get = Mock(return_value=None)
        
        # When
        result = await cache_service.get("missing_key")
        
        # Then
        assert result is None
    
    @pytest.mark.asyncio
    async def test_should_delete_key(self, cache_service, redis_mock):
        """åº”è¯¥åˆ é™¤é”®"""
        # Given
        redis_mock.delete = Mock(return_value=1)
        
        # When
        deleted = await cache_service.delete("test_key")
        
        # Then
        assert deleted is True
        redis_mock.delete.assert_called_once_with("test_key")
    
    # ============ TTLæµ‹è¯• ============
    
    @pytest.mark.asyncio
    async def test_should_expire_after_ttl(self, cache_service, redis_mock):
        """åº”è¯¥åœ¨TTLåè¿‡æœŸ"""
        # Given
        redis_mock.setex = Mock(return_value=True)
        
        # When
        await cache_service.set("temp_key", "value", ttl=60)
        
        # Then
        redis_mock.setex.assert_called_once_with("temp_key", 60, '"value"')
    
    @pytest.mark.asyncio
    async def test_should_refresh_ttl_on_access(self, cache_service, redis_mock):
        """è®¿é—®æ—¶åº”è¯¥åˆ·æ–°TTL"""
        # Given
        redis_mock.get = Mock(return_value='"value"')
        redis_mock.expire = Mock(return_value=True)
        
        # When
        await cache_service.get_and_refresh("key", ttl=300)
        
        # Then
        redis_mock.expire.assert_called_once_with("key", 300)
    
    # ============ æ‰¹é‡æ“ä½œæµ‹è¯• ============
    
    @pytest.mark.asyncio
    async def test_should_batch_get(self, cache_service, redis_mock):
        """åº”è¯¥æ‰¹é‡è·å–"""
        # Given
        keys = ["key1", "key2", "key3"]
        values = ['"val1"', '"val2"', None]
        redis_mock.mget = Mock(return_value=values)
        
        # When
        results = await cache_service.mget(keys)
        
        # Then
        assert results == {"key1": "val1", "key2": "val2", "key3": None}
    
    @pytest.mark.asyncio
    async def test_should_batch_set(self, cache_service, redis_mock):
        """åº”è¯¥æ‰¹é‡è®¾ç½®"""
        # Given
        data = {"key1": "val1", "key2": "val2"}
        redis_mock.mset = Mock(return_value=True)
        
        # When
        await cache_service.mset(data, ttl=300)
        
        # Then
        redis_mock.mset.assert_called_once()
    
    # ============ ç¼“å­˜ç­–ç•¥æµ‹è¯• ============
    
    @pytest.mark.asyncio
    async def test_cache_aside_pattern(self, cache_service, redis_mock):
        """æµ‹è¯•Cache-Asideæ¨¡å¼"""
        # Given
        redis_mock.get = Mock(return_value=None)  # ç¼“å­˜æœªå‘½ä¸­
        redis_mock.set = Mock(return_value=True)
        
        async def load_from_db():
            return {"id": 1, "data": "from_db"}
        
        # When
        result = await cache_service.get_or_load(
            "cache_key",
            load_from_db,
            ttl=600
        )
        
        # Then
        assert result == {"id": 1, "data": "from_db"}
        redis_mock.set.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_should_prevent_cache_stampede(self, cache_service):
        """åº”è¯¥é˜²æ­¢ç¼“å­˜é›ªå´©"""
        # Given
        call_count = 0
        
        async def expensive_operation():
            nonlocal call_count
            call_count += 1
            await asyncio.sleep(0.1)
            return f"result_{call_count}"
        
        # When - å¹¶å‘è¯·æ±‚ç›¸åŒçš„é”®
        tasks = [
            cache_service.get_or_load(
                "same_key",
                expensive_operation,
                ttl=60
            )
            for _ in range(10)
        ]
        
        results = await asyncio.gather(*tasks)
        
        # Then - åªåº”è¯¥è°ƒç”¨ä¸€æ¬¡
        assert call_count == 1
        assert all(r == "result_1" for r in results)
```

---

## å››ã€APIå¥‘çº¦æµ‹è¯•

### 4.1 å¥‘çº¦å®šä¹‰

```python
# tests/contracts/document_api_contract.py
from pydantic import BaseModel, Field
from typing import List, Optional
from datetime import datetime


class DocumentUploadRequest(BaseModel):
    """æ–‡æ¡£ä¸Šä¼ è¯·æ±‚å¥‘çº¦"""
    file: bytes = Field(..., description="æ–‡ä»¶å†…å®¹")
    name: str = Field(..., min_length=1, max_length=255)
    type: str = Field(..., pattern="^(pdf|docx|txt|md)$")
    metadata: Optional[dict] = Field(default={})


class DocumentUploadResponse(BaseModel):
    """æ–‡æ¡£ä¸Šä¼ å“åº”å¥‘çº¦"""
    id: str
    status: str = Field(..., pattern="^(pending|processing|completed|failed)$")
    message: str
    created_at: datetime


class SearchRequest(BaseModel):
    """æœç´¢è¯·æ±‚å¥‘çº¦"""
    query: str = Field(..., min_length=1, max_length=500)
    top_k: int = Field(default=10, ge=1, le=100)
    threshold: float = Field(default=0.7, ge=0.0, le=1.0)
    filters: Optional[dict] = None


class SearchResponse(BaseModel):
    """æœç´¢å“åº”å¥‘çº¦"""
    results: List[dict]
    total: int
    took_ms: float
```

### 4.2 å¥‘çº¦æµ‹è¯•

```python
# tests/test_api_contracts.py
import pytest
from fastapi.testclient import TestClient
from unittest.mock import patch

from app.main import app
from tests.contracts.document_api_contract import *


class TestAPIContracts:
    """APIå¥‘çº¦æµ‹è¯•"""
    
    @pytest.fixture
    def client(self):
        """åˆ›å»ºæµ‹è¯•å®¢æˆ·ç«¯"""
        return TestClient(app)
    
    def test_document_upload_contract(self, client):
        """æµ‹è¯•æ–‡æ¡£ä¸Šä¼ å¥‘çº¦"""
        # Given
        request_data = {
            "name": "test.pdf",
            "type": "pdf",
            "metadata": {"source": "test"}
        }
        
        files = {
            "file": ("test.pdf", b"PDF content", "application/pdf")
        }
        
        # When
        response = client.post(
            "/api/v1/documents",
            data=request_data,
            files=files
        )
        
        # Then
        assert response.status_code == 201
        
        # éªŒè¯å“åº”å¥‘çº¦
        data = response.json()
        validated = DocumentUploadResponse(**data)
        
        assert validated.id
        assert validated.status in ["pending", "processing"]
        assert validated.created_at
    
    def test_search_contract(self, client):
        """æµ‹è¯•æœç´¢å¥‘çº¦"""
        # Given
        request = SearchRequest(
            query="test query",
            top_k=5,
            threshold=0.8
        )
        
        # When
        response = client.post(
            "/api/v1/search",
            json=request.dict()
        )
        
        # Then
        assert response.status_code == 200
        
        # éªŒè¯å“åº”å¥‘çº¦
        data = response.json()
        validated = SearchResponse(**data)
        
        assert isinstance(validated.results, list)
        assert validated.total >= 0
        assert validated.took_ms >= 0
    
    def test_contract_validation_error(self, client):
        """æµ‹è¯•å¥‘çº¦éªŒè¯é”™è¯¯"""
        # Given - æ— æ•ˆçš„è¯·æ±‚
        invalid_request = {
            "query": "",  # ç©ºæŸ¥è¯¢
            "top_k": 1000,  # è¶…å‡ºèŒƒå›´
            "threshold": 2.0  # æ— æ•ˆé˜ˆå€¼
        }
        
        # When
        response = client.post(
            "/api/v1/search",
            json=invalid_request
        )
        
        # Then
        assert response.status_code == 422
        errors = response.json()["detail"]
        
        # åº”è¯¥åŒ…å«æ‰€æœ‰éªŒè¯é”™è¯¯
        error_fields = [e["loc"][-1] for e in errors]
        assert "query" in error_fields
        assert "top_k" in error_fields
        assert "threshold" in error_fields
```

---

## äº”ã€æ€§èƒ½æµ‹è¯•

### 5.1 è´Ÿè½½æµ‹è¯•

```python
# tests/performance/load_test.py
import asyncio
import aiohttp
import time
from statistics import mean, stdev
from typing import List
import pytest


class LoadTester:
    """è´Ÿè½½æµ‹è¯•å™¨"""
    
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.results = []
    
    async def single_request(self, session: aiohttp.ClientSession) -> dict:
        """æ‰§è¡Œå•ä¸ªè¯·æ±‚"""
        start = time.time()
        
        try:
            async with session.post(
                f"{self.base_url}/api/v1/search",
                json={"query": "test query", "top_k": 10}
            ) as response:
                await response.json()
                latency = (time.time() - start) * 1000
                
                return {
                    "status": response.status,
                    "latency_ms": latency,
                    "success": response.status == 200
                }
        except Exception as e:
            return {
                "status": 0,
                "latency_ms": (time.time() - start) * 1000,
                "success": False,
                "error": str(e)
            }
    
    async def run_load_test(
        self,
        concurrent_users: int,
        duration_seconds: int
    ) -> dict:
        """è¿è¡Œè´Ÿè½½æµ‹è¯•"""
        async with aiohttp.ClientSession() as session:
            end_time = time.time() + duration_seconds
            tasks = []
            
            while time.time() < end_time:
                # åˆ›å»ºå¹¶å‘è¯·æ±‚
                batch = [
                    self.single_request(session)
                    for _ in range(concurrent_users)
                ]
                
                results = await asyncio.gather(*batch)
                self.results.extend(results)
                
                # çŸ­æš‚å»¶è¿Ÿé¿å…è¿‡è½½
                await asyncio.sleep(0.1)
        
        return self.analyze_results()
    
    def analyze_results(self) -> dict:
        """åˆ†ææµ‹è¯•ç»“æœ"""
        successful = [r for r in self.results if r["success"]]
        failed = [r for r in self.results if not r["success"]]
        latencies = [r["latency_ms"] for r in successful]
        
        return {
            "total_requests": len(self.results),
            "successful": len(successful),
            "failed": len(failed),
            "success_rate": len(successful) / len(self.results) * 100,
            "latency": {
                "mean": mean(latencies) if latencies else 0,
                "stdev": stdev(latencies) if len(latencies) > 1 else 0,
                "min": min(latencies) if latencies else 0,
                "max": max(latencies) if latencies else 0,
                "p50": self.percentile(latencies, 50),
                "p95": self.percentile(latencies, 95),
                "p99": self.percentile(latencies, 99)
            }
        }
    
    def percentile(self, data: List[float], p: int) -> float:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        if not data:
            return 0
        sorted_data = sorted(data)
        index = int(len(sorted_data) * p / 100)
        return sorted_data[min(index, len(sorted_data) - 1)]


@pytest.mark.performance
class TestLoadPerformance:
    """è´Ÿè½½æ€§èƒ½æµ‹è¯•"""
    
    @pytest.mark.asyncio
    async def test_normal_load(self):
        """æµ‹è¯•æ­£å¸¸è´Ÿè½½"""
        tester = LoadTester("http://localhost:8000")
        
        results = await tester.run_load_test(
            concurrent_users=10,
            duration_seconds=30
        )
        
        # éªŒè¯æ€§èƒ½æŒ‡æ ‡
        assert results["success_rate"] > 99
        assert results["latency"]["p95"] < 500  # 95%è¯·æ±‚å°äº500ms
        assert results["latency"]["p99"] < 1000  # 99%è¯·æ±‚å°äº1s
    
    @pytest.mark.asyncio
    async def test_peak_load(self):
        """æµ‹è¯•å³°å€¼è´Ÿè½½"""
        tester = LoadTester("http://localhost:8000")
        
        results = await tester.run_load_test(
            concurrent_users=100,
            duration_seconds=60
        )
        
        # å³°å€¼è´Ÿè½½ä¸‹çš„æ€§èƒ½è¦æ±‚
        assert results["success_rate"] > 95
        assert results["latency"]["p95"] < 2000
        assert results["latency"]["p99"] < 5000
    
    @pytest.mark.asyncio
    async def test_stress_test(self):
        """å‹åŠ›æµ‹è¯• - æ‰¾å‡ºç³»ç»Ÿæé™"""
        tester = LoadTester("http://localhost:8000")
        
        for users in [10, 50, 100, 200, 500]:
            results = await tester.run_load_test(
                concurrent_users=users,
                duration_seconds=10
            )
            
            print(f"\nå¹¶å‘ç”¨æˆ·: {users}")
            print(f"æˆåŠŸç‡: {results['success_rate']:.2f}%")
            print(f"P95å»¶è¿Ÿ: {results['latency']['p95']:.2f}ms")
            
            # æ‰¾åˆ°ç³»ç»Ÿå´©æºƒç‚¹
            if results["success_rate"] < 90:
                print(f"ç³»ç»Ÿåœ¨ {users} å¹¶å‘ç”¨æˆ·æ—¶å¼€å§‹é™çº§")
                break
```

### 5.2 å†…å­˜å’ŒCPUæµ‹è¯•

```python
# tests/performance/resource_test.py
import psutil
import threading
import time
import pytest


class ResourceMonitor:
    """èµ„æºç›‘æ§å™¨"""
    
    def __init__(self):
        self.monitoring = False
        self.samples = []
        self.process = psutil.Process()
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True
        self.samples = []
        
        def monitor():
            while self.monitoring:
                self.samples.append({
                    "timestamp": time.time(),
                    "cpu_percent": self.process.cpu_percent(),
                    "memory_mb": self.process.memory_info().rss / 1024 / 1024,
                    "threads": self.process.num_threads(),
                    "connections": len(self.process.connections())
                })
                time.sleep(0.1)
        
        thread = threading.Thread(target=monitor)
        thread.start()
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        time.sleep(0.2)
    
    def get_stats(self):
        """è·å–ç»Ÿè®¡æ•°æ®"""
        if not self.samples:
            return {}
        
        cpu_values = [s["cpu_percent"] for s in self.samples]
        memory_values = [s["memory_mb"] for s in self.samples]
        
        return {
            "cpu": {
                "max": max(cpu_values),
                "avg": sum(cpu_values) / len(cpu_values)
            },
            "memory": {
                "max": max(memory_values),
                "avg": sum(memory_values) / len(memory_values),
                "leak": memory_values[-1] - memory_values[0]
            },
            "threads": {
                "max": max(s["threads"] for s in self.samples)
            }
        }


@pytest.mark.resource
class TestResourceUsage:
    """èµ„æºä½¿ç”¨æµ‹è¯•"""
    
    def test_memory_usage_under_load(self):
        """æµ‹è¯•è´Ÿè½½ä¸‹çš„å†…å­˜ä½¿ç”¨"""
        monitor = ResourceMonitor()
        monitor.start_monitoring()
        
        # æ¨¡æ‹Ÿå¤„ç†å¤§é‡æ–‡æ¡£
        for _ in range(100):
            # å¤„ç†æ–‡æ¡£çš„ä»£ç 
            pass
        
        monitor.stop_monitoring()
        stats = monitor.get_stats()
        
        # éªŒè¯èµ„æºä½¿ç”¨
        assert stats["memory"]["max"] < 500  # æœ€å¤§å†…å­˜å°äº500MB
        assert stats["memory"]["leak"] < 50   # å†…å­˜æ³„æ¼å°äº50MB
        assert stats["cpu"]["avg"] < 80       # å¹³å‡CPUä½¿ç”¨ç‡å°äº80%
```

---

## å…­ã€CI/CDé›†æˆ

### 6.1 GitHub Actionsé…ç½®

```yaml
# .github/workflows/backend-test.yml
name: Backend Tests

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
          
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
          
      milvus:
        image: milvusdb/milvus:v2.3.0
        ports:
          - 19530:19530
    
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          
      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-test.txt
          
      - name: Run linting
        run: |
          flake8 app/ --max-line-length=100
          black app/ --check
          mypy app/
          
      - name: Run unit tests
        run: |
          pytest tests/unit -v --cov=app --cov-report=xml
          
      - name: Run integration tests
        run: |
          pytest tests/integration -v
        env:
          DATABASE_URL: postgresql://postgres:test@localhost/test
          REDIS_URL: redis://localhost:6379
          
      - name: Run contract tests
        run: |
          pytest tests/contracts -v
          
      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage.xml
          flags: backend
          
      - name: Run performance tests
        if: github.event_name == 'pull_request'
        run: |
          pytest tests/performance -v -m "not stress"
          
      - name: Generate test report
        if: always()
        run: |
          pytest --html=report.html --self-contained-html
          
      - name: Upload test artifacts
        if: failure()
        uses: actions/upload-artifact@v3
        with:
          name: test-results
          path: |
            report.html
            coverage.xml
            .coverage
```

### 6.2 æµ‹è¯•è´¨é‡é—¨ç¦

```yaml
# sonar-project.properties
sonar.projectKey=rag-system-backend
sonar.sources=app
sonar.tests=tests
sonar.python.coverage.reportPaths=coverage.xml
sonar.python.xunit.reportPath=test-results/*.xml

# è´¨é‡é—¨ç¦æ ‡å‡†
sonar.qualitygate.wait=true
sonar.coverage.threshold=85
sonar.duplications.threshold=3
sonar.issues.threshold=0
```

---

## ä¸ƒã€æµ‹è¯•æœ€ä½³å®è·µæ€»ç»“

### 7.1 æµ‹è¯•ç¼–å†™åŸåˆ™

```python
# tests/best_practices.md
"""
åç«¯TDDæœ€ä½³å®è·µ

1. æµ‹è¯•ç‹¬ç«‹æ€§
   - æ¯ä¸ªæµ‹è¯•ç‹¬ç«‹è¿è¡Œ
   - ä¸ä¾èµ–æµ‹è¯•æ‰§è¡Œé¡ºåº
   - ä½¿ç”¨fixtureç®¡ç†æµ‹è¯•æ•°æ®

2. æµ‹è¯•å‘½å
   - test_should_[expected_behavior]_when_[condition]
   - æ¸…æ™°æè¿°æµ‹è¯•æ„å›¾

3. æµ‹è¯•ç»“æ„
   - Arrange: å‡†å¤‡æµ‹è¯•æ•°æ®
   - Act: æ‰§è¡Œè¢«æµ‹ä»£ç 
   - Assert: éªŒè¯ç»“æœ

4. Mockä½¿ç”¨
   - åªMockå¤–éƒ¨ä¾èµ–
   - ä¿æŒMockç®€å•
   - éªŒè¯Mockè°ƒç”¨

5. æ€§èƒ½è€ƒè™‘
   - å•å…ƒæµ‹è¯• < 100ms
   - é›†æˆæµ‹è¯• < 1s
   - ä½¿ç”¨å¹¶è¡Œæ‰§è¡Œ

6. æ•°æ®ç®¡ç†
   - ä½¿ç”¨å·¥å‚æ¨¡å¼åˆ›å»ºæµ‹è¯•æ•°æ®
   - æµ‹è¯•åæ¸…ç†æ•°æ®
   - é¿å…ç¡¬ç¼–ç 
"""
```

### 7.2 TDDå·¥ä½œæµ

```mermaid
graph TD
    A[éœ€æ±‚åˆ†æ] --> B[ç¼–å†™æµ‹è¯•ç”¨ä¾‹]
    B --> C[è¿è¡Œæµ‹è¯•-å¤±è´¥]
    C --> D[ç¼–å†™æœ€å°å®ç°]
    D --> E[è¿è¡Œæµ‹è¯•-é€šè¿‡]
    E --> F[é‡æ„ä»£ç ]
    F --> G[è¿è¡Œæµ‹è¯•-ç¡®ä¿é€šè¿‡]
    G --> H{æ‰€æœ‰éœ€æ±‚å®Œæˆ?}
    H -->|å¦| B
    H -->|æ˜¯| I[ä»£ç è¯„å®¡]
    I --> J[åˆå¹¶ä»£ç ]
```

---

è¿™ä»½å®Œæ•´çš„TDDè®¾è®¡æ–‡æ¡£æ¶µç›–äº†å‰åç«¯çš„æµ‹è¯•ç­–ç•¥ã€å®æ–½æ–¹æ³•å’Œæœ€ä½³å®è·µã€‚å…³é”®è¦ç‚¹ï¼š

1. **æµ‹è¯•å…ˆè¡Œ**ï¼šå…ˆå†™æµ‹è¯•ï¼Œå†å†™ä»£ç 
2. **åˆ†å±‚æµ‹è¯•**ï¼šå•å…ƒã€é›†æˆã€å¥‘çº¦ã€æ€§èƒ½
3. **è‡ªåŠ¨åŒ–**ï¼šCI/CDé›†æˆï¼Œè´¨é‡é—¨ç¦
4. **å¯ç»´æŠ¤**ï¼šæ¸…æ™°çš„å‘½åï¼Œç‹¬ç«‹çš„æµ‹è¯•
5. **é«˜è¦†ç›–**ï¼šæ ¸å¿ƒåŠŸèƒ½100%ï¼Œæ•´ä½“85%+

å»ºè®®å›¢é˜Ÿä¸¥æ ¼æ‰§è¡ŒTDDæµç¨‹ï¼Œç¡®ä¿ä»£ç è´¨é‡å’Œç³»ç»Ÿç¨³å®šæ€§ã€‚